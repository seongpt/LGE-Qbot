{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 모듈 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pythoncom\n",
    "import os, time, io, pptx\n",
    "import pandas as pd\n",
    "\n",
    "from pptxtopdf import convert\n",
    "from pptx import Presentation\n",
    "\n",
    "import mysql.connector\n",
    "import yaml\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import DocumentAnalysisFeature\n",
    "from azure.ai.documentintelligence.models import ContentFormat\n",
    "import re, pdfplumber\n",
    "import tiktoken\n",
    "from openai import AzureOpenAI\n",
    "import json, time, base64\n",
    "import fitz\n",
    "from tqdm import tqdm\n",
    "import pickle, requests\n",
    "from collections import defaultdict, Counter\n",
    "from azure.storage.blob import BlobServiceClient, generate_blob_sas, BlobSasPermissions, ContentSettings\n",
    "from azure.core.exceptions import *\n",
    "\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.utils.cell import range_boundaries\n",
    "from copy import copy\n",
    "from PIL import Image\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib.pagesizes import letter\n",
    "\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # FutureWarning 제거\n",
    "\n",
    "# 임시 함수들\n",
    "from temp_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. config 정보 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('configs\\\\config.yml', 'r', encoding='UTF-8') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "mysql_config = config['mysql_dev']   # 개발 DB 연결 정보\n",
    "blob_config  = config['blob_storage_dev']   # 개발 Blob Storage 연결 정보\n",
    "blob_config_prod2  = config['blob_storage_prod2']   # 운영 Blob Storage 연결 정보\n",
    "di_config = config['di']   # Document Intelligence 연결 정보\n",
    "aoai_config  = config['aoai']['gpt_4_0409']   # GPT 리소스 정보\n",
    "ss_vars      = config['section_split_vars']\n",
    "gpt_split_vars = ss_vars['gpt_split']\n",
    "blob_service_client  = BlobServiceClient.from_connection_string(blob_config['connect_string'])\n",
    "blob_service_client_prod2  = BlobServiceClient.from_connection_string(blob_config_prod2['connect_string'])\n",
    "\n",
    "DI_ENDPOINT = di_config['endpoint']\n",
    "DI_KEY      = di_config['key']\n",
    "DI_API_VERSION = di_config['api_version']\n",
    "\n",
    "document_intelligence_client = DocumentIntelligenceClient(\n",
    "        endpoint    = DI_ENDPOINT,\n",
    "        credential  = AzureKeyCredential(DI_KEY),\n",
    "        api_version = DI_API_VERSION\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 전처리 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_container_client = blob_service_client.get_container_client(container=\"prep-data\")\n",
    "\n",
    "\n",
    "## HTML SAS URL 생성 함수\n",
    "def get_sas_url(blob_config, container_client, blob_path, expiry_time=True) :\n",
    "\n",
    "    blob_client = container_client.get_blob_client(blob_path)\n",
    "    if expiry_time :\n",
    "        kst = timezone(timedelta(hours=9))\n",
    "        start_time_kst = datetime.now(kst) - timedelta(minutes=5)\n",
    "\n",
    "        expiry_time_kst = start_time_kst + timedelta(days=1)\n",
    "        start_time_utc = start_time_kst.astimezone(timezone.utc)\n",
    "        expiry_time_utc = expiry_time_kst.astimezone(timezone.utc)\n",
    "    \n",
    "    else :\n",
    "        kst = timezone(timedelta(hours=9))\n",
    "        start_time_kst = datetime.now(kst) - timedelta(minutes=5)\n",
    "        start_time_utc = start_time_kst.astimezone(timezone.utc)\n",
    "        expiry_time_utc = datetime(9999, 12, 31)\n",
    "\n",
    "    try: \n",
    "        sas_token = generate_blob_sas(\n",
    "            account_name=blob_config['account_name'],\n",
    "            account_key=blob_config['account_key'],\n",
    "            container_name=container_client.container_name,\n",
    "            blob_name=blob_path,\n",
    "            permission=BlobSasPermissions(read=True, write=True, delete=True, add=True, create=True, update=True, process=True),\n",
    "            start=start_time_utc,\n",
    "            expiry=expiry_time_utc                  \n",
    "        )\n",
    "        sas_url = blob_client.url + '?' + sas_token\n",
    "        return sas_url\n",
    "\n",
    "    except Exception as e:\n",
    "        sas_url = None  \n",
    "        return None\n",
    "    \n",
    "\n",
    "## 현재 섹션 분할 대상 페이지의 Paragraphs만 추출하는 함수\n",
    "def get_page_paragraphs (paragraphs, pageNum, except_texts) :\n",
    "    pageParagraphs = []\n",
    "    \n",
    "    for para in paragraphs :\n",
    "        if para.bounding_regions[0]['pageNumber'] == pageNum :\n",
    "            content = para.content\n",
    "            \n",
    "            for except_text in except_texts:\n",
    "                if except_text in content:\n",
    "                    content = content.replace(except_text, '')\n",
    "            \n",
    "            content = re.sub(r'[^가-힣a-zA-Z0-9\\s]', '', content)\n",
    "            content = content.replace(' ', '').replace('\\n', '').replace('\\\\', '') \\\n",
    "                    .replace('/', '').strip()\n",
    "                    \n",
    "            if 'Q봇학습제외' in content:\n",
    "                return None\n",
    "            \n",
    "            pageParagraphs.append(para)\n",
    "            \n",
    "    return pageParagraphs\n",
    "\n",
    "\n",
    "## 현재 섹션 분할 대상 페이지의 Table을 Markdown으로 변환하는 함수\n",
    "def get_page_table (tables, pageNum, header_x=0.0) :\n",
    "    txtMarkdown = f\"\"\n",
    "    tablesMD = []\n",
    "    cells_offset = []\n",
    "\n",
    "    for table in tables :\n",
    "        if table['boundingRegions'][0]['polygon'][0] > header_x:\n",
    "            tabe_pageNum = table.get(\"boundingRegions\", [])[0].get(\"pageNumber\")\n",
    "            if tabe_pageNum == pageNum :\n",
    "                txtMarkdown = f\"|\"\n",
    "                totRowCnt = table.row_count\n",
    "                totColCnt = table.column_count\n",
    "\n",
    "                preRow = 0\n",
    "                for rIdx in range(totRowCnt) :\n",
    "                    if preRow != rIdx :\n",
    "                        txtMarkdown = txtMarkdown + \"\\n|\"\n",
    "                    for cIdx in range(totColCnt) :\n",
    "                        cell = next((cell for cell in table['cells'] if cell['rowIndex'] == rIdx and cell['columnIndex'] == cIdx), None)\n",
    "                        if cell != None :\n",
    "                            content = cell.content.replace('\\n', '')\n",
    "                            txtMarkdown = txtMarkdown + f\"{content}\"\n",
    "                            if cell['spans'] != [] :\n",
    "                                cells_offset.append(cell['spans'][0]['offset'])\n",
    "                                \n",
    "                        txtMarkdown = txtMarkdown + \"|\"\n",
    "                        \n",
    "                txtMarkdown = txtMarkdown + f\"\\n\"\n",
    "                \n",
    "                tablesMD.append({'pageNumber':tabe_pageNum, 'spans':table.spans, 'cells':table.cells,'boundingRegions':table.bounding_regions,'markdown':txtMarkdown})\n",
    "            \n",
    "    return tablesMD, cells_offset\n",
    "\n",
    "\n",
    "## Blob Storage에 섹션명으로 된 폴더가 있는 확인하는 함수\n",
    "def folder_exists_in_azure(base_path, section_name):\n",
    "    # 폴더명 리스트 가져오기 위해 blob 이름 확인\n",
    "    folder_names = set()\n",
    "    blobs = prep_container_client.list_blobs(name_starts_with=base_path)\n",
    "    for blob in blobs:\n",
    "        blob_name = blob.name[len(base_path):]  # base_path 이후의 부분만 확인\n",
    "        parts = blob_name.split('/')  # '/' 기준으로 분할\n",
    "        if len(parts) > 1:  # 2개 이상이면 하위 폴더를 의미\n",
    "            folder_names.add(parts[0])\n",
    "\n",
    "    # section_name과 일치하는 폴더가 있는지 확인\n",
    "    return any(name for name in folder_names if name == section_name), folder_names\n",
    "\n",
    "\n",
    "## 시리즈 값들을 ','로 연결하는 함수\n",
    "def join_strings(series):\n",
    "    flat_list = [item for sublist in series.dropna() for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "    unique_series = set(filter(None, map(str, flat_list)))\n",
    "    return ','.join(filter(None, unique_series))\n",
    "\n",
    "\n",
    "## 헤더에서 제품의 MIN, MAX 사이즈 정보 추출하는 함수\n",
    "def extract_min_max(input_string):\n",
    "    # '숫자~숫자' 형태에서 숫자와 선택적인 단위 패턴 찾기\n",
    "    range_pattern = re.compile(r'(\\d+)(kg|인치|mm|km|L)?~(\\d+)(kg|인치|mm|km|L)?')\n",
    "    \n",
    "    # '숫자단위숫자단위' 형태에서 단위가 끼어 있는 경우 패턴 찾기\n",
    "    intermediate_range_pattern = re.compile(r'(\\d+)(kg|인치|mm|km|L)(\\d+)(kg|인치|mm|km|L)')\n",
    "\n",
    "    # 모든 범위의 최소값과 최대값을 추출하기 위한 리스트\n",
    "    min_values = []\n",
    "    max_values = []\n",
    "\n",
    "    # 단위가 있는 범위 추출\n",
    "    matches = range_pattern.findall(input_string)\n",
    "    for match in matches:\n",
    "        min_val = int(match[0])\n",
    "        max_val = int(match[2])\n",
    "        min_values.append(min_val)\n",
    "        max_values.append(max_val)\n",
    "\n",
    "    # 단위가 끼어 있는 범위 추출\n",
    "    intermediate_matches = intermediate_range_pattern.findall(input_string)\n",
    "    for match in intermediate_matches:\n",
    "        if match[1] == match[3]:  # 동일 단위일 경우\n",
    "            min_val = int(match[0])\n",
    "            max_val = int(match[2])\n",
    "            min_values.append(min_val)\n",
    "            max_values.append(max_val)\n",
    "\n",
    "    # 최소값 중 최소값, 최대값 중 최대값을 계산\n",
    "    min_common = min(min_values) if min_values else None\n",
    "    max_common = max(max_values) if max_values else None\n",
    "\n",
    "    return min_common, max_common\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 텍스트 추출 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM TB_PREP_SOURCE\n",
    "WHERE 1=1\n",
    "    AND PREP_TYPE_CD = 'PRIOR_INST'\n",
    "    AND PREP_STATUS_CD = 'N'\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "tmp_df = select_table(mysql_config, query)\n",
    "tmp_df['DI_RUNNING_TIME'] = None  # 실행시간 컬럼 추가\n",
    "\n",
    "\n",
    "raw_container_client = blob_service_client.get_container_client(container=\"raw-data\")\n",
    "pkl_container_client = blob_service_client.get_container_client(container=\"pkl-data\")\n",
    "\n",
    "# pptx를 pdf로 변환 및 pkl 업로드에 필요한 로컬 폴더 생성\n",
    "os.makedirs('DI', exist_ok=True)\n",
    "os.makedirs('pkl', exist_ok=True)\n",
    "\n",
    "for i,row in tmp_df.iterrows():\n",
    "    file_path = row['SOURCE_FILE_PATH']\n",
    "    file_name = row['FILE_NM']\n",
    "    file_name_without_ext = os.path.splitext(file_name)[0]\n",
    "    file_ext = os.path.splitext(file_name)[1]\n",
    "\n",
    "    # pkl 폴더의 모든 파일 삭제\n",
    "    for filename in os.listdir('pkl'):\n",
    "        pkl_path = os.path.join('pkl', filename)\n",
    "        try:\n",
    "            if os.path.isfile(pkl_path):\n",
    "                os.unlink(pkl_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {pkl_path}. Reason: {e}\")\n",
    "\n",
    "    print('##############################################################')\n",
    "    print(datetime.now())\n",
    "    print(f\"{i} : {file_path}/{file_name}\")\n",
    "    print('##############################################################')\n",
    "    \n",
    "    # 엑셀이면 패스\n",
    "    if file_ext == '.xlsx' or file_ext == '.xls':\n",
    "        continue\n",
    "    \n",
    "    # 확장자가 pptx면 pdf로 변환 후 저장\n",
    "    if file_ext == '.pptx' or file_ext == '.ppt':\n",
    "        pythoncom.CoInitialize()\n",
    "        \n",
    "        for filename in os.listdir('DI'):\n",
    "            pdf_path = os.path.join('DI', filename)\n",
    "            try:\n",
    "                if os.path.isfile(pdf_path):\n",
    "                    os.unlink(pdf_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete {pdf_path}. Reason: {e}\")\n",
    "        \n",
    "        try:\n",
    "            file_client = raw_container_client.get_blob_client(blob=f\"{file_path}/{file_name}\")\n",
    "            \n",
    "            # PPTX 파일을 로컬에 다운로드\n",
    "            with open(f\"DI\\\\{file_name}\", \"wb\") as file:\n",
    "                file.write(file_client.download_blob().readall())\n",
    "                \n",
    "            # PPTX 파일을 PDF로 변환 후 저장\n",
    "            convert(f\"DI\\\\{file_name}\", f\"{os.getcwd()}\\\\DI\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('pass')\n",
    "        \n",
    "        try:\n",
    "            pdf_file_client = raw_container_client.get_blob_client(blob=f\"{file_path}/{file_name_without_ext}.pdf\")\n",
    "            with open(f\"DI\\\\{file_name_without_ext}.pdf\", \"rb\") as f:\n",
    "                pdf_file_client.upload_blob(f, overwrite=True, content_settings=ContentSettings(content_type=\"application/pdf\"))\n",
    "                \n",
    "            file_name = f\"{file_name_without_ext}.pdf\"\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "        \n",
    "        pythoncom.CoUninitialize()\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        sas_url = get_sas_url(blob_config, container_client=raw_container_client, blob_path=f'{file_path}/{file_name}', expiry_time=False)\n",
    "\n",
    "        with urlopen(sas_url) as f:\n",
    "            pdf_data = f.read()\n",
    "            \n",
    "            # DI 사용하는 경우 \n",
    "            poller = document_intelligence_client.begin_analyze_document(\n",
    "                \"prebuilt-layout\",\n",
    "                analyze_request=pdf_data,\n",
    "                features=[DocumentAnalysisFeature.KEY_VALUE_PAIRS],# , DocumentAnalysisFeature.LANGUAGES],\n",
    "                content_type=\"application/octet-stream\",\n",
    "                output_content_format = ContentFormat.MARKDOWN # ContentFormat.TEXT \n",
    "            )\n",
    "\n",
    "        result = poller.result()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        execution_time_seconds = end_time - start_time  # 소스코드 실행 시간 계산 (초 단위)\n",
    "        print(execution_time_seconds)\n",
    "        print(file_name)\n",
    "        \n",
    "        tmp_df.loc[i, 'DI_RUNNING_TIME'] = execution_time_seconds\n",
    "            \n",
    "        with open(f\"pkl\\\\{file_name_without_ext}.pkl\", \"wb\") as f: # 한 번 돌려서 pickle 파일로 저장\n",
    "            pickle.dump(result, f)\n",
    "            \n",
    "        blob_client = pkl_container_client.get_blob_client(blob=f\"{file_path}/{file_name_without_ext}.pkl\")\n",
    "        \n",
    "        with open(f\"pkl\\\\{file_name_without_ext}.pkl\", \"rb\") as data:\n",
    "            blob_client.upload_blob(data, overwrite=True, content_settings=ContentSettings(content_type=\"application/octet-stream\"))\n",
    "            \n",
    "        update_query = f\"\"\"\n",
    "            UPDATE TB_PREP_SOURCE\n",
    "            SET PREP_STATUS_CD = 'C'\n",
    "                , UPDATE_PGM = 'PRIOR_INST 전처리'\n",
    "                , UPDATE_DTTM = SYSDATE()\n",
    "            WHERE 1=1\n",
    "                AND IF_CATEGORY_CD = '{row['IF_CATEGORY_CD']}'\n",
    "                AND CONTENTS_ID    = '{row['CONTENTS_ID']}'\n",
    "                AND ITEM_ID        = '{row['ITEM_ID']}'\n",
    "            ;\n",
    "        \"\"\"\n",
    "        commit_query(mysql_config, update_query)\n",
    "        print('Compelete!!!!!!!!!!!!!!!!')\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"{str(e)}\")\n",
    "        \n",
    "        error_msg = f\"{str(e)}\"\n",
    "        \n",
    "        update_query = f\"\"\"\n",
    "            UPDATE TB_PREP_SOURCE\n",
    "            SET PREP_STATUS_CD = 'E'\n",
    "                , PREP_ERROR_MSG = '{error_msg}'\n",
    "                , UPDATE_PGM = 'PRIOR_INST 전처리'\n",
    "                , UPDATE_DTTM = SYSDATE()\n",
    "            WHERE 1=1\n",
    "                AND IF_CATEGORY_CD = '{row['IF_CATEGORY_CD']}'\n",
    "                AND CONTENTS_ID    = '{row['CONTENTS_ID']}'\n",
    "                AND ITEM_ID        = '{row['ITEM_ID']}'\n",
    "            ;\n",
    "        \"\"\"\n",
    "        commit_query(mysql_config, update_query)\n",
    "        continue\n",
    "    \n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 섹션 분할 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_container_client = blob_service_client.get_container_client(container=\"raw-data\")\n",
    "prep_container_client = blob_service_client.get_container_client(container=\"prep-data\")\n",
    "web_client = blob_service_client_prod2.get_container_client(container=\"$web\")\n",
    "\n",
    "## 전처리 대상 load \n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    *\n",
    "FROM TB_PREP_SOURCE\n",
    "WHERE 1=1\n",
    "    AND PREP_TYPE_CD = 'PRIOR_INST'\n",
    "    AND PREP_STATUS_CD = 'C'\n",
    ";\n",
    "\"\"\"\n",
    "tb_if_prep_files_source = select_table(mysql_config, query)\n",
    "\n",
    "## pkl 블롭 파일 목록\n",
    "pkl_container_client = blob_service_client.get_container_client(container=\"pkl-data\")\n",
    "pkl_blob_list = [i for i in pkl_container_client.list_blob_names() if \".pkl\" in i]\n",
    "## 테이블 활용해서 pkl 파일 위치 찾기\n",
    "pkl_path_df = tb_if_prep_files_source[['IF_CATEGORY_CD', 'CONTENTS_ID', 'ITEM_ID'\n",
    "                                       ,'FILE_NM', 'SOURCE_FILE_PATH', 'RESULT_FILE_PATH'\n",
    "                                       , 'PRODUCT_LVL1_CD', 'PRODUCT_LVL2_CD', 'PRODUCT_LVL3_CD'\n",
    "                                       , 'PROD_MODEL_CD', 'SALES_MODEL_CD']].drop_duplicates()\n",
    "\n",
    "## 제외 Text load\n",
    "query = f\"\"\"\n",
    "SELECT TRIM(EXCEPT_TEXT_VAL) AS EXCEPT_TEXT_VAL\n",
    "FROM TB_EXCEPT_TEXT_MST\n",
    "WHERE USE_FLAG = 'Y'\n",
    "ORDER BY LENGTH(EXCEPT_TEXT_VAL) DESC\n",
    ";\n",
    "\"\"\"\n",
    "TB_MST_PREP_EXCEPT_TEXT = select_table(mysql_config, query)\n",
    "except_texts = TB_MST_PREP_EXCEPT_TEXT['EXCEPT_TEXT_VAL'].to_list()\n",
    "\n",
    "\n",
    "# 제품정보 테이블에서 lvl2의 lvl3 정보 모두 SELECT\n",
    "model_map_query = f\"\"\"\n",
    "    SELECT DISTINCT PRODUCT_LVL1_CD, PRODUCT_LVL2_NM, PRODUCT_LVL2_CD, PRODUCT_LVL3_CD, PRODUCT_LVL3_NM\n",
    "    FROM TB_MODEL_MAP_MST\n",
    "    WHERE 1=1\n",
    "        AND SET_MODEL_YN = 'N'\n",
    "    ;\n",
    "\"\"\"\n",
    "model_map_df = select_table(mysql_config, model_map_query)\n",
    "\n",
    "pkl_path_df['CT_SECTION_SPLIT_RUNNING_TIME'] = None\n",
    "\n",
    "\n",
    "# 파일 하나씩 loop\n",
    "for idx in tqdm(pkl_path_df.index):\n",
    "    error_msg = ''\n",
    "    \n",
    "    idx_row  = pkl_path_df.loc[idx]\n",
    "    category_cd = idx_row['IF_CATEGORY_CD']\n",
    "    contents_id = idx_row['CONTENTS_ID']\n",
    "    item_id = idx_row['ITEM_ID']\n",
    "    prod_model_cd = idx_row['PROD_MODEL_CD']\n",
    "    sales_model_cd = idx_row['SALES_MODEL_CD']\n",
    "    \n",
    "    file_nm  = idx_row['FILE_NM']\n",
    "    file_nm_no_ext = os.path.splitext(file_nm)[0]\n",
    "    file_type = file_nm.split('.')[-1]\n",
    "\n",
    "    src_fpath = idx_row['SOURCE_FILE_PATH']\n",
    "    result_path = idx_row['RESULT_FILE_PATH']\n",
    "\n",
    "    lvl1_cd = idx_row['PRODUCT_LVL1_CD']\n",
    "    lvl2_cd = idx_row['PRODUCT_LVL2_CD']\n",
    "    lvl3_cd = idx_row['PRODUCT_LVL3_CD']\n",
    "    \n",
    "    print('##############################################################')\n",
    "    print(f'################### {src_fpath}/{file_nm} ###################')\n",
    "    print('##############################################################')\n",
    "    \n",
    "    cur_pkl_blob_list = [pkl_blob for pkl_blob in pkl_blob_list if f\"{src_fpath}/{file_nm_no_ext}.pkl\" == pkl_blob]\n",
    "    if (len(cur_pkl_blob_list) == 0) | (len(cur_pkl_blob_list) > 1) :\n",
    "        print(f\"pickle 파일이 없거나 너무 많습니다.\")\n",
    "        print(f\"PKL FILE LIST : {cur_pkl_blob_list}\")\n",
    "        continue\n",
    "    \n",
    "    ## Load Pkl\n",
    "    pkl_blob_client = pkl_container_client.get_blob_client(f\"{src_fpath}/{file_nm_no_ext}.pkl\")\n",
    "    pkl_db_bytes   = pkl_blob_client.download_blob().readall()\n",
    "    pkl_blob_stream = io.BytesIO(pkl_db_bytes)\n",
    "    di_result = pickle.load(pkl_blob_stream)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        sas_url = get_sas_url(blob_config, container_client=pdf_container_client, blob_path=f\"{src_fpath}/{file_nm_no_ext}.pdf\", expiry_time=False)\n",
    "        request_blob = requests.get(sas_url)\n",
    "        filestream = io.BytesIO(request_blob.content)\n",
    "        doc = pdfplumber.open(filestream)\n",
    "        doc2 = fitz.open(stream=filestream, filetype=\"pdf\")\n",
    "    except:\n",
    "        sas_url = get_sas_url(blob_config, container_client=pdf_container_client, blob_path=f\"{src_fpath}/{file_nm_no_ext}.PDF\", expiry_time=False)\n",
    "        request_blob = requests.get(sas_url)\n",
    "        filestream = io.BytesIO(request_blob.content)\n",
    "        doc = pdfplumber.open(filestream)\n",
    "        doc2 = fitz.open(stream=filestream, filetype=\"pdf\")\n",
    "        \n",
    "    # 결과를 insert할 데이터프레임 구조 정의\n",
    "    query = f\"\"\"\n",
    "    SELECT COLUMN_NAME\n",
    "    FROM INFORMATION_SCHEMA.COLUMNS\n",
    "    WHERE TABLE_NAME = 'TB_PREP_RESULT_STG'\n",
    "    AND TABLE_SCHEMA = 'QBOTDB'\n",
    "    ORDER BY ORDINAL_POSITION ASC;\n",
    "    \"\"\"\n",
    "    result_df = select_table(mysql_config, query)\n",
    "    result_list = result_df['COLUMN_NAME'].to_list()\n",
    "    df_data = {col: [None]*len(doc.pages) for col in result_list}\n",
    "    result_df = pd.DataFrame(df_data)\n",
    "    \n",
    "    # result_df에 고정값 세팅\n",
    "    result_df['IF_CATEGORY_CD'] = category_cd\n",
    "    result_df['CONTENTS_ID'] = contents_id\n",
    "    result_df['ITEM_ID'] = item_id\n",
    "    result_df['FILE_NM'] = file_nm_no_ext\n",
    "    result_df['MODEL_USE_FLAG'] = 'Y'\n",
    "    result_df['PRODUCT_LVL1_CD'] = lvl1_cd\n",
    "    result_df['PRODUCT_LVL2_CD'] = lvl2_cd\n",
    "    result_df['PRODUCT_LVL3_CD'] = lvl3_cd\n",
    "    result_df['PROD_MODEL_CD'] = prod_model_cd\n",
    "    result_df['SALES_MODEL_CD'] = sales_model_cd\n",
    "    result_df['IF_FLAG'] = 'N'\n",
    "    result_df['CREATE_DTTM'] = datetime.now()\n",
    "    result_df['CREATE_PGM'] = '3차 전처리'\n",
    "    result_df['IF_TYPE_CD'] = 'C'\n",
    "    \n",
    "    # 기존에 해당 파일을 전처리 했으면 blob에서 삭제\n",
    "    result_file_blobs = prep_container_client.list_blobs(name_starts_with=f'{result_path}/{file_nm_no_ext}/')\n",
    "    for result_file_blob in result_file_blobs:\n",
    "        result_blob_client = prep_container_client.get_blob_client(result_file_blob)\n",
    "        result_blob_client.delete_blob()\n",
    "\n",
    "    header_y = 0.5\n",
    "    header_x = 1.5\n",
    "    result_df_idx = 0\n",
    "     \n",
    "    prc_str_time = time.time()\n",
    "    \n",
    "    # 페이지별로 loop\n",
    "    for page in di_result.pages:\n",
    "        saveText = \"\"\n",
    "        saveText_bytes = None\n",
    "        saveText_base64 = None\n",
    "        pageContents = []\n",
    "        \n",
    "        min_val = None\n",
    "        max_val = None\n",
    "\n",
    "        sectionName = None\n",
    "        sectionFileName = None\n",
    "        leftmost_pageheader = None\n",
    "        leftmost_x = float('inf')\n",
    "        \n",
    "        pageNum = page.page_number\n",
    "        pageParagraphs = get_page_paragraphs(di_result.paragraphs, pageNum, except_texts)\n",
    "        \n",
    "        print('==============================================================')\n",
    "        print(f'================== 현재 pageNum : {pageNum} ==================')\n",
    "        print('==============================================================')\n",
    "        \n",
    "        if pageParagraphs == [] or pageParagraphs == None:\n",
    "            continue\n",
    "            \n",
    "        # PageHeader 추출\n",
    "        for para in pageParagraphs:\n",
    "            if para['boundingRegions'][0]['polygon'][1] < header_y:\n",
    "                # polygon의 왼쪽 끝 x 좌표 구하기\n",
    "                left_x = para['boundingRegions'][0]['polygon'][0]\n",
    "\n",
    "                # 왼쪽 끝 PageHeader 업데이트\n",
    "                if left_x < leftmost_x:\n",
    "                    leftmost_pageheader = para\n",
    "                    leftmost_x = left_x\n",
    "        \n",
    "        pageheader = leftmost_pageheader['content']\n",
    "        \n",
    "        for except_text in except_texts:\n",
    "            if except_text in pageheader:\n",
    "                pageheader = pageheader.replace(except_text, '')\n",
    "        \n",
    "        pageheader = re.sub(r'^>|>$', '', pageheader)\n",
    "        pageheader = pageheader.replace('\\n', '').replace('\\\\', '') \\\n",
    "                .replace('/', '').replace('<', '').replace('!', '').replace('-', '').replace('=', '').replace('\"', '').strip()\n",
    "\n",
    "        if '>' in pageheader:\n",
    "            lst = [element.strip() for element in pageheader.split('>')]\n",
    "\n",
    "            # min, max 값 추출\n",
    "            min_val, max_val = extract_min_max(lst[-1])\n",
    "\n",
    "            for idx, item in enumerate(lst):\n",
    "                if '_' in item:\n",
    "                    input_idx = idx\n",
    "                    input = item\n",
    "                    break\n",
    "\n",
    "            if '_' in input:\n",
    "                tmp = pd.DataFrame({'PAGE' : [pageNum], \"PRODUCT_LVL2_NM\" : [input]})\n",
    "                tmp['PRODUCT_LVL2_NM'] = tmp['PRODUCT_LVL2_NM'].str.split(',')\n",
    "                tmp = tmp.explode('PRODUCT_LVL2_NM')\n",
    "                tmp['PRODUCT_LVL2_NM'] = tmp['PRODUCT_LVL2_NM'].str.split(\"_\").str.get(0).str.strip()\n",
    "                \n",
    "                result = tmp.groupby(['PAGE'], as_index=False).agg({'PRODUCT_LVL2_NM' : join_strings})\n",
    "                \n",
    "                pageheader_new = \"\"\n",
    "                idx = 0\n",
    "                for text in pageheader.split('>'):\n",
    "                    if idx == input_idx:\n",
    "                        text = result['PRODUCT_LVL2_NM'].values[0]\n",
    "                \n",
    "                    idx += 1\n",
    "                \n",
    "                    pageheader_new = \" \".join([pageheader_new, text.strip()])\n",
    "                \n",
    "                pageheader_new = pageheader_new.strip()\n",
    "\n",
    "                sectionName = pageheader_new\n",
    "                sectionName = sectionName.replace('이전 설치 접수 가이드', '')\n",
    "                \n",
    "                sectionFileName = pageheader.split('>')[-1].strip()\n",
    "                sectionFileName = sectionFileName.replace('이전 설치 접수 가이드', '')\n",
    "\n",
    "            else:\n",
    "                sectionFileName = pageheader.split('>')[-1]\n",
    "                sectionFileName = sectionFileName.replace('이전 설치 접수 가이드', '')\n",
    "                sectionFileName = sectionFileName.replace('OBS 업무 Guide', '')\n",
    "\n",
    "                sectionName = ' '.join(pageheader.split('>'))\n",
    "                sectionName = re.sub(r'\\s+', ' ', sectionName)\n",
    "                sectionName = sectionName.replace('이전 설치 접수 가이드', '')\n",
    "                sectionName = sectionName.replace('OBS 업무 Guide', '')\n",
    "\n",
    "        else:\n",
    "            pageheader = re.sub(r'\\s+', ' ', pageheader)\n",
    "            sectionFileName = pageheader\n",
    "            sectionFileName = sectionFileName.replace('이전 설치 접수 가이드', '')\n",
    "            sectionFileName = sectionFileName.replace('OBS 업무 Guide', '')\n",
    "            \n",
    "            sectionName = pageheader\n",
    "            sectionName = sectionName.replace('이전 설치 접수 가이드', '')\n",
    "            sectionName = sectionName.replace('OBS 업무 Guide', '')\n",
    "        \n",
    "        # 섹션 분할\n",
    "        pageContents.append(f\"{pageNum} : {sectionFileName}\")\n",
    "        \n",
    "        if di_result.tables != None:\n",
    "            tables, cells_offset = get_page_table(di_result.tables, pageNum, header_x)\n",
    "        \n",
    "        for para in pageParagraphs :\n",
    "            para_1st_x = para['boundingRegions'][0]['polygon'][0]\n",
    "            para_1st_y = para['boundingRegions'][0]['polygon'][1]\n",
    "            \n",
    "            if para_1st_y > header_y:\n",
    "                if para['spans'] != []:\n",
    "                    para_offset = para['spans'][0]['offset']\n",
    "                    \n",
    "                    if para_1st_x >= header_x:\n",
    "                        if len(tables) > 0 :\n",
    "                            if tables[0]['spans'] != []:\n",
    "                                table_offset = tables[0]['spans'][0]['offset']\n",
    "                                if para_offset >= table_offset :\n",
    "                                    pageContents.append(tables[0]['markdown'])\n",
    "                                    del tables[0]\n",
    "                            \n",
    "                        if para_offset not in cells_offset:\n",
    "                            pageContents.append(para['content'])\n",
    "                    \n",
    "        for i in range(len(pageContents)) :\n",
    "            content = pageContents[i]\n",
    "            \n",
    "            for except_text in except_texts:\n",
    "                if except_text in content:\n",
    "                    content = content.replace(except_text, '')\n",
    "                    \n",
    "            saveText = saveText + content + '\\n'\n",
    "            pageContents[i] = content\n",
    "        \n",
    "        ## Blob 저장 (저장할 Text)\n",
    "        exists, folder_names = folder_exists_in_azure(f\"{result_path}/{file_nm_no_ext}/\", sectionName)\n",
    "        \n",
    "        if exists:\n",
    "            folder_name = f'{result_path}/{file_nm_no_ext}/' + [name for name in folder_names if name == sectionName][0]\n",
    "        else:\n",
    "            folder_name = f\"{result_path}/{file_nm_no_ext}/{sectionName}\"\n",
    "            \n",
    "        save_blob_name = f\"{folder_name}/{str(pageNum).zfill(3)}_{sectionFileName}.txt\"\n",
    "        print(save_blob_name)\n",
    "        \n",
    "        ## result_df에 필요한 정보 추가\n",
    "        result_df.loc[result_df_idx, 'SECTION_NM'] = folder_name.split('/')[-1]\n",
    "        result_df.loc[result_df_idx, 'SECTION_FILE_NM'] = save_blob_name.split('/')[-1]\n",
    "        result_df.loc[result_df_idx, 'FULL_FILE_PATH'] = save_blob_name\n",
    "        result_df.loc[result_df_idx, 'SEQ_ID'] = pageNum\n",
    "        result_df.loc[result_df_idx, 'PAGE_NUM'] = pageNum\n",
    "        result_df.loc[result_df_idx, 'PAGE_NUMS'] = pageNum\n",
    "        result_df.loc[result_df_idx, 'SIZE_MIN'] = min_val\n",
    "        result_df.loc[result_df_idx, 'SIZE_MAX'] = max_val\n",
    "            \n",
    "        # 각 페이지 txt 파일 적재  --> Blob 에 적재\n",
    "        blob_client = prep_container_client.get_blob_client(blob=save_blob_name)\n",
    "        blob_client.upload_blob(saveText, overwrite=True)\n",
    "        \n",
    "        saveText_bytes = saveText.encode('utf-8')\n",
    "        saveText_base64 = base64.b64encode(saveText_bytes).decode('utf-8')\n",
    "        result_df.loc[result_df_idx, 'RESULT_TEXT'] = saveText_base64\n",
    "        \n",
    "        # 각 페이지에서 image가 차지하는 비율 계산\n",
    "        target = doc.pages[pageNum-1]\n",
    "        \n",
    "        text_length_sum = 0\n",
    "        image_sum = 0\n",
    "        image_ratio = 0\n",
    "        \n",
    "        # 페이지 넓이 계산\n",
    "        page_area = round(target.width*target.height, 2)\n",
    "        \n",
    "        # 텍스트 개수 계산\n",
    "        for texts in target.extract_words():\n",
    "            text = texts['text']\n",
    "            length = len(text)\n",
    "            text_length_sum += length\n",
    "            \n",
    "        result_df.loc[result_df_idx, 'TEXT_CNT'] = text_length_sum\n",
    "        \n",
    "        if target.images != []:\n",
    "            # Image 넓이 계산\n",
    "            for images in target.images:\n",
    "                width = round(images['width'], 2)\n",
    "                height = round(images['height'], 2)\n",
    "                area = round(width*height, 2)\n",
    "                image_sum += area\n",
    "            image_sum = round(image_sum, 2)\n",
    "            image_ratio = round(image_sum / page_area, 2)\n",
    "            \n",
    "            result_df.loc[result_df_idx, 'IMAGE_RATIO'] = image_ratio\n",
    "\n",
    "            caption_start = []\n",
    "            pad = 15\n",
    "            dpi = 200\n",
    "            seq = 1\n",
    "            \n",
    "            # 캡션 텍스트 좌표 확인\n",
    "            for text in target.extract_words():\n",
    "                if '[이미지_' in text['text']:\n",
    "                    caption_start.append([text['x0'], text['bottom'], text['text']])\n",
    "            \n",
    "            try:\n",
    "                # 페이지별 이미지 추출\n",
    "                for image in target.images:\n",
    "                    for i in range(len(caption_start)):\n",
    "                        start_x = caption_start[i][0]\n",
    "                        start_y = caption_start[i][1]\n",
    "                        tag_name = caption_start[i][2]\n",
    "                        \n",
    "                        if (start_x-pad <= image['x0'] <= start_x+pad) and (start_y <= image['top'] <= start_y+pad):\n",
    "                            page = doc2[pageNum-1]\n",
    "                            page.set_cropbox([image['x0'], image['top'], image['x1'], image['bottom']])\n",
    "                            pixmap = page.get_pixmap(matrix=fitz.Matrix(dpi/72, dpi/72))\n",
    "                            # PNG 형식으로 이미지 데이터 추출\n",
    "                            img_data = pixmap.tobytes(\"png\")\n",
    "                            # Blob Storage에 업로드\n",
    "                            img_blob_client = prep_container_client.get_blob_client(f\"{folder_name}/images/{str(pageNum).zfill(3)}_{str(seq).zfill(3)}_{tag_name}.png\")\n",
    "                            img_blob_client.upload_blob(img_data, overwrite=True, content_type=\"image/png\")\n",
    "                            \n",
    "                            seq += 1\n",
    "            except Exception as e:\n",
    "                print(f\"{str(e)}\")\n",
    "                \n",
    "                result_df_idx += 1\n",
    "                continue\n",
    "                    \n",
    "        result_df_idx += 1\n",
    "\n",
    "    doc2.close()\n",
    "        \n",
    "    ## PAGENUM이 NULL인 행 삭제\n",
    "    result_df = result_df.dropna(subset=['PAGE_NUM'])\n",
    "\n",
    "    ## HTML 생성\n",
    "    try:\n",
    "        sas_url = get_sas_url(blob_config, container_client=pdf_container_client, blob_path=f\"{src_fpath}/{file_nm_no_ext}.pdf\", expiry_time=False)\n",
    "        request_blob = requests.get(sas_url)\n",
    "        filestream = io.BytesIO(request_blob.content)\n",
    "        doc = fitz.open(stream=filestream, filetype=\"pdf\")\n",
    "    except:\n",
    "        sas_url = get_sas_url(blob_config, container_client=pdf_container_client, blob_path=f\"{src_fpath}/{file_nm_no_ext}.PDF\", expiry_time=False)\n",
    "        request_blob = requests.get(sas_url)\n",
    "        filestream = io.BytesIO(request_blob.content)\n",
    "        doc = fitz.open(stream=filestream, filetype=\"pdf\")\n",
    "\n",
    "    # pdf 저장 경로 사용해서 적재하는경우\n",
    "    save_folder = f'{result_path}/{file_nm_no_ext}'\n",
    "\n",
    "    for i, page in tqdm(enumerate(doc)):\n",
    "        svg = page.get_svg_image(matrix=fitz.Identity)\n",
    "        svg_bytes  = svg.encode('utf-8')\n",
    "        img_data = io.BytesIO(svg_bytes)\n",
    "        img_data.seek(0)\n",
    "        img_blob_client = web_client.get_blob_client(f\"{save_folder}/images/{i+1}.svg\")\n",
    "        img_blob_client.upload_blob(img_data, content_type=\"image/svg+xml\", overwrite=True)\n",
    "    doc.close()\n",
    "\n",
    "    img_path       = f\"{save_folder}/images/\"\n",
    "    img_bname_list = web_client.list_blobs(name_starts_with = img_path) # 적재된 image 목록 \n",
    "    img_bname_list = [i.name for i in img_bname_list if i.name.endswith('.svg')]\n",
    "\n",
    "    if len(img_bname_list) == 0:\n",
    "        print(\"적재된 Image 파일 없음\")\n",
    "    else : \n",
    "        blob_client = web_client.get_blob_client(f'{result_path}/{file_nm_no_ext}')\n",
    "        root_folder = f\"{blob_client.url}/images\"\n",
    "\n",
    "        html_content = \"<html><head></head><body style='background-color: white;'></body>\\n\"\n",
    "        html_content += \"<script>\\n\"\n",
    "        \n",
    "        if file_type == 'ppt' or file_type == 'pptx':\n",
    "            html_content += f\"\"\"\n",
    "const rootFolder = '{root_folder}'\n",
    "const sas_key = 'sv=2023-11-03&st=2024-07-24T08%3A08%3A12Z&se=9999-12-31T12%3A00%3A00Z&sr=c&sp=rl&sig=vfHfN9ax60XVqxsZ5ae2VUjAomSwqZGKOLrptQiDKko%3D'\n",
    "const hash = window.location.hash;\n",
    "const page = hash.match(/#page(\\d+)/)[1];\n",
    "\n",
    "if(page) {{\n",
    "    let pageInt = parseInt(page)\n",
    "    let startPage = pageInt - 5;\n",
    "    let endPage = pageInt + 5;\n",
    "\n",
    "    let body = document.body;\n",
    "    for ( i = startPage; i <= endPage; i++) {{\n",
    "\n",
    "        if(i >= 1){{\n",
    "            \n",
    "            div = document.createElement('div');\n",
    "            div.setAttribute('id', 'page'+i);\n",
    "            div.setAttribute('style', 'text-align: center;');\n",
    "\n",
    "            let img = document.createElement('img');\n",
    "            let src = rootFolder+'/'+i+'.svg'+'?'+sas_key\n",
    "            img.setAttribute('src', src)\n",
    "            img.setAttribute('style', 'display: inline-block; margin:10px auto;width:1500px;');\n",
    "            img.onerror = function() {{\n",
    "                img.remove();\n",
    "            }};\n",
    "            \n",
    "            if(img) {{\n",
    "                div.appendChild(img);\n",
    "                body.appendChild(div);\n",
    "            }}\n",
    "        }}\n",
    "    }}\n",
    "}}\n",
    "            \"\"\"\n",
    "\n",
    "        else:\n",
    "            \n",
    "            html_content += f\"\"\"\n",
    "const rootFolder = '{root_folder}'\n",
    "const sas_key = 'sv=2023-11-03&st=2024-07-24T08%3A08%3A12Z&se=9999-12-31T12%3A00%3A00Z&sr=c&sp=rl&sig=vfHfN9ax60XVqxsZ5ae2VUjAomSwqZGKOLrptQiDKko%3D'\n",
    "const hash = window.location.hash;\n",
    "const page = hash.match(/#page(\\d+)/)[1];\n",
    "\n",
    "if(page) {{\n",
    "    let pageInt = parseInt(page)\n",
    "    let startPage = pageInt - 5;\n",
    "    let endPage = pageInt + 5;\n",
    "\n",
    "    let body = document.body;\n",
    "    for ( i = startPage; i <= endPage; i++) {{\n",
    "\n",
    "        if(i >= 1){{\n",
    "            \n",
    "            div = document.createElement('div');\n",
    "            div.setAttribute('id', 'page'+i);\n",
    "            div.setAttribute('style', 'text-align: center;');\n",
    "\n",
    "            let img = document.createElement('img');\n",
    "            let src = rootFolder+'/'+i+'.svg'+'?'+sas_key\n",
    "            img.setAttribute('src', src)\n",
    "            img.setAttribute('style', 'display: inline-block; margin:10px auto;width:70%;');\n",
    "            img.onerror = function() {{\n",
    "                img.remove();\n",
    "            }};\n",
    "            \n",
    "            if(img) {{\n",
    "                div.appendChild(img);\n",
    "                body.appendChild(div);\n",
    "            }}\n",
    "        }}\n",
    "    }}\n",
    "}}\n",
    "            \"\"\"\n",
    "\n",
    "        html_content += \"\\n</script></html>\"\n",
    "            \n",
    "        html_bname = f\"{save_folder}/{file_nm_no_ext}.html\"\n",
    "        html_blob_client = web_client.get_blob_client(html_bname)\n",
    "        html_blob_client.upload_blob(html_content, content_type=(\"text/html\"), overwrite=True)\n",
    "\n",
    "        ## SAS Link 생성\n",
    "        html_sas_url = get_sas_url(blob_config_prod2, web_client, html_bname, expiry_time=False)\n",
    "\n",
    "        for result_idx, result_row in result_df.iterrows():\n",
    "            pnum = result_row['PAGE_NUM']\n",
    "            result_df.loc[result_idx, 'HTML_SAS_URL'] = html_sas_url + '#page' + str(pnum)\n",
    "        \n",
    "    prc_running_time = time.time() - prc_str_time\n",
    "    \n",
    "    pkl_path_df.loc[idx, 'CT_SECTION_SPLIT_RUNNING_TIME'] = prc_running_time\n",
    "\n",
    "    try:\n",
    "        # INSERT RESULT_STG_BACKUP\n",
    "        backup_query = f\"\"\"\n",
    "            INSERT INTO TB_PREP_RESULT_STG_BACKUP\n",
    "            (BACKUP_DTTM, IF_CATEGORY_CD, CONTENTS_ID, ITEM_ID, SEQ_ID, FILE_NM, SECTION_NM,\n",
    "            SECTION_FILE_NM, PAGE_NUM, PAGE_NUMS, FULL_FILE_PATH, HTML_SAS_URL, MODEL_USE_FLAG, PRODUCT_LVL1_CD,\n",
    "            PRODUCT_LVL2_CD, PRODUCT_LVL3_CD, PROD_MODEL_CD, SALES_MODEL_CD, SYMP_CODE_ONE, SYMP_CODE_TWO, SYMP_CODE_THREE, IF_FLAG, IF_DTTM,\n",
    "            CREATE_DTTM, CREATE_PGM, UPDATE_DTTM, UPDATE_PGM, RESULT_TEXT, IMAGE_RATIO,\n",
    "            TEXT_CNT, SIZE_MIN, SIZE_MAX, DOCS_DATE, IF_TYPE_CD)\n",
    "            SELECT SYSDATE() AS BACKUP_DTTM,\n",
    "            IF_CATEGORY_CD, CONTENTS_ID, ITEM_ID, SEQ_ID, FILE_NM, SECTION_NM,\n",
    "            SECTION_FILE_NM, PAGE_NUM, PAGE_NUMS, FULL_FILE_PATH, HTML_SAS_URL, MODEL_USE_FLAG, PRODUCT_LVL1_CD,\n",
    "            PRODUCT_LVL2_CD, PRODUCT_LVL3_CD, PROD_MODEL_CD, SALES_MODEL_CD, SYMP_CODE_ONE, SYMP_CODE_TWO, SYMP_CODE_THREE, IF_FLAG, IF_DTTM,\n",
    "            CREATE_DTTM, CREATE_PGM, UPDATE_DTTM, UPDATE_PGM, RESULT_TEXT, IMAGE_RATIO,\n",
    "            TEXT_CNT, SIZE_MIN, SIZE_MAX, DOCS_DATE, IF_TYPE_CD\n",
    "            FROM TB_PREP_RESULT_STG\n",
    "            WHERE 1=1\n",
    "                AND IF_CATEGORY_CD = '{idx_row['IF_CATEGORY_CD']}'\n",
    "                AND CONTENTS_ID    = '{idx_row['CONTENTS_ID']}'\n",
    "                AND ITEM_ID        = '{idx_row['ITEM_ID']}'\n",
    "            ;\n",
    "        \"\"\"\n",
    "        commit_query(mysql_config, backup_query)\n",
    "        \n",
    "        # DELETE RESULT_STG\n",
    "        delete_query = f\"\"\"\n",
    "            DELETE\n",
    "            FROM TB_PREP_RESULT_STG\n",
    "            WHERE 1=1\n",
    "                AND IF_CATEGORY_CD = '{idx_row['IF_CATEGORY_CD']}'\n",
    "                AND CONTENTS_ID    = '{idx_row['CONTENTS_ID']}'\n",
    "                AND ITEM_ID        = '{idx_row['ITEM_ID']}'\n",
    "            ;\n",
    "        \"\"\"\n",
    "        commit_query(mysql_config, delete_query)\n",
    "        \n",
    "        \n",
    "        # INSERT RESULT_STG\n",
    "        query = f\"\"\"\n",
    "            INSERT INTO TB_PREP_RESULT_STG\n",
    "            (IF_CATEGORY_CD, CONTENTS_ID, ITEM_ID, SEQ_ID, FILE_NM, SECTION_NM,\n",
    "            SECTION_FILE_NM, PAGE_NUM, PAGE_NUMS, FULL_FILE_PATH, HTML_SAS_URL, MODEL_USE_FLAG, PRODUCT_LVL1_CD,\n",
    "            PRODUCT_LVL2_CD, PRODUCT_LVL3_CD, PROD_MODEL_CD, SALES_MODEL_CD, SYMP_CODE_ONE, SYMP_CODE_TWO, SYMP_CODE_THREE, IF_FLAG, IF_DTTM,\n",
    "            CREATE_DTTM, CREATE_PGM, UPDATE_DTTM, UPDATE_PGM, RESULT_TEXT, IMAGE_RATIO,\n",
    "            TEXT_CNT, SIZE_MIN, SIZE_MAX, DOCS_DATE, IF_TYPE_CD)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s,\n",
    "            %s, %s, %s, %s, %s, %s, %s,\n",
    "            %s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
    "            %s, %s, %s, %s, FROM_BASE64(%s), %s,\n",
    "            %s, %s, %s, %s, %s)\n",
    "            ;\n",
    "        \"\"\"\n",
    "        vals = list(result_df.itertuples(index=False, name=None))\n",
    "        commit_query_w_vals(mysql_config, query, vals, True)\n",
    "        \n",
    "        # UPDATE SOURCE\n",
    "        update_query = f\"\"\"\n",
    "            UPDATE TB_PREP_SOURCE\n",
    "            SET PREP_STATUS_CD = 'Y'\n",
    "                , UPDATE_PGM = 'PRIOR_INST 전처리'\n",
    "                , UPDATE_DTTM = SYSDATE()\n",
    "            WHERE 1=1\n",
    "                AND IF_CATEGORY_CD = '{idx_row['IF_CATEGORY_CD']}'\n",
    "                AND CONTENTS_ID    = '{idx_row['CONTENTS_ID']}'\n",
    "                AND ITEM_ID        = '{idx_row['ITEM_ID']}'\n",
    "            ;\n",
    "        \"\"\"\n",
    "        commit_query(mysql_config, update_query)\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_msg += f\"{str(e)}\\n\"\n",
    "        \n",
    "        print('DB INSERT 실패')\n",
    "        update_query = f\"\"\"\n",
    "            UPDATE TB_PREP_SOURCE\n",
    "            SET PREP_STATUS_CD = 'E'\n",
    "                , PREP_ERROR_MSG = '{error_msg}'\n",
    "                , UPDATE_PGM = 'PRIOR_INST 전처리'\n",
    "                , UPDATE_DTTM = SYSDATE()\n",
    "            WHERE 1=1\n",
    "                AND IF_CATEGORY_CD = '{idx_row['IF_CATEGORY_CD']}'\n",
    "                AND CONTENTS_ID    = '{idx_row['CONTENTS_ID']}'\n",
    "                AND ITEM_ID        = '{idx_row['ITEM_ID']}'\n",
    "            ;\n",
    "        \"\"\"\n",
    "        commit_query(mysql_config, update_query)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
