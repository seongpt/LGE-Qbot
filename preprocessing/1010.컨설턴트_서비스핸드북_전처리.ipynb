{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 모듈 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pythoncom\n",
    "import os, time, io, pptx\n",
    "import pandas as pd\n",
    "\n",
    "from pptxtopdf import convert\n",
    "from pptx import Presentation\n",
    "\n",
    "import mysql.connector\n",
    "import yaml\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import DocumentAnalysisFeature\n",
    "from azure.ai.documentintelligence.models import ContentFormat\n",
    "import re, pdfplumber\n",
    "import tiktoken\n",
    "from openai import AzureOpenAI\n",
    "import json, time, base64\n",
    "import fitz\n",
    "from tqdm import tqdm\n",
    "import pickle, requests\n",
    "from collections import defaultdict, Counter\n",
    "from azure.storage.blob import BlobServiceClient, generate_blob_sas, BlobSasPermissions, ContentSettings\n",
    "from azure.core.exceptions import *\n",
    "\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.utils.cell import range_boundaries\n",
    "from copy import copy\n",
    "from PIL import Image\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib.pagesizes import letter\n",
    "\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # FutureWarning 제거\n",
    "\n",
    "# 임시 함수들\n",
    "from temp_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. config 정보 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('configs\\\\config.yml', 'r', encoding='UTF-8') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "mysql_config = config['mysql_dev']   # 개발 DB 연결 정보\n",
    "blob_config  = config['blob_storage_dev']   # 개발 Blob Storage 연결 정보\n",
    "blob_config_prod2  = config['blob_storage_prod2']   # 운영 Blob Storage 연결 정보\n",
    "di_config = config['di']   # Document Intelligence 연결 정보\n",
    "aoai_config  = config['aoai']['gpt_4_0409']   # GPT 리소스 정보\n",
    "ss_vars      = config['section_split_vars']\n",
    "gpt_split_vars = ss_vars['gpt_split']\n",
    "blob_service_client  = BlobServiceClient.from_connection_string(blob_config['connect_string'])\n",
    "blob_service_client_prod2  = BlobServiceClient.from_connection_string(blob_config_prod2['connect_string'])\n",
    "\n",
    "DI_ENDPOINT = di_config['endpoint']\n",
    "DI_KEY      = di_config['key']\n",
    "DI_API_VERSION = di_config['api_version']\n",
    "\n",
    "document_intelligence_client = DocumentIntelligenceClient(\n",
    "        endpoint    = DI_ENDPOINT,\n",
    "        credential  = AzureKeyCredential(DI_KEY),\n",
    "        api_version = DI_API_VERSION\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 전처리 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_container_client = blob_service_client.get_container_client(container=\"prep-data\")\n",
    "\n",
    "\n",
    "## HTML SAS URL 생성 함수\n",
    "def get_sas_url(blob_config, container_client, blob_path, expiry_time=True) :\n",
    "\n",
    "    blob_client = container_client.get_blob_client(blob_path)\n",
    "    if expiry_time :\n",
    "        kst = timezone(timedelta(hours=9))\n",
    "        start_time_kst = datetime.now(kst) - timedelta(minutes=5)\n",
    "\n",
    "        expiry_time_kst = start_time_kst + timedelta(days=1)\n",
    "        start_time_utc = start_time_kst.astimezone(timezone.utc)\n",
    "        expiry_time_utc = expiry_time_kst.astimezone(timezone.utc)\n",
    "    \n",
    "    else :\n",
    "        kst = timezone(timedelta(hours=9))\n",
    "        start_time_kst = datetime.now(kst) - timedelta(minutes=5)\n",
    "        start_time_utc = start_time_kst.astimezone(timezone.utc)\n",
    "        expiry_time_utc = datetime(9999, 12, 31)\n",
    "\n",
    "    try: \n",
    "        sas_token = generate_blob_sas(\n",
    "            account_name=blob_config['account_name'],\n",
    "            account_key=blob_config['account_key'],\n",
    "            container_name=container_client.container_name,\n",
    "            blob_name=blob_path,\n",
    "            permission=BlobSasPermissions(read=True, write=True, delete=True, add=True, create=True, update=True, process=True),\n",
    "            start=start_time_utc,\n",
    "            expiry=expiry_time_utc                  \n",
    "        )\n",
    "        sas_url = blob_client.url + '?' + sas_token\n",
    "        return sas_url\n",
    "\n",
    "    except Exception as e:\n",
    "        sas_url = None  \n",
    "        return None\n",
    "    \n",
    "\n",
    "## 현재 섹션 분할 대상 페이지의 Paragraphs만 추출하는 함수\n",
    "def get_page_paragraphs (paragraphs, pageNum, except_texts) :\n",
    "    pageParagraphs = []\n",
    "    \n",
    "    for para in paragraphs :\n",
    "        if para.bounding_regions[0]['pageNumber'] == pageNum :\n",
    "            content = para.content\n",
    "            \n",
    "            for except_text in except_texts:\n",
    "                if except_text in content:\n",
    "                    content = content.replace(except_text, '')\n",
    "            \n",
    "            content = re.sub(r'[^가-힣a-zA-Z0-9\\s]', '', content)\n",
    "            content = content.replace(' ', '').replace('\\n', '').replace('\\\\', '') \\\n",
    "                    .replace('/', '').strip()\n",
    "                    \n",
    "            if 'Q봇학습제외' in content:\n",
    "                return None\n",
    "            \n",
    "            pageParagraphs.append(para)\n",
    "            \n",
    "    return pageParagraphs\n",
    "\n",
    "\n",
    "## 현재 섹션 분할 대상 페이지의 Table을 Markdown으로 변환하는 함수\n",
    "def get_page_table (tables, pageNum, header_x=0.0) :\n",
    "    txtMarkdown = f\"\"\n",
    "    tablesMD = []\n",
    "    cells_offset = []\n",
    "\n",
    "    for table in tables :\n",
    "        if table['boundingRegions'][0]['polygon'][0] > header_x:\n",
    "            tabe_pageNum = table.get(\"boundingRegions\", [])[0].get(\"pageNumber\")\n",
    "            if tabe_pageNum == pageNum :\n",
    "                txtMarkdown = f\"|\"\n",
    "                totRowCnt = table.row_count\n",
    "                totColCnt = table.column_count\n",
    "\n",
    "                preRow = 0\n",
    "                for rIdx in range(totRowCnt) :\n",
    "                    if preRow != rIdx :\n",
    "                        txtMarkdown = txtMarkdown + \"\\n|\"\n",
    "                    for cIdx in range(totColCnt) :\n",
    "                        cell = next((cell for cell in table['cells'] if cell['rowIndex'] == rIdx and cell['columnIndex'] == cIdx), None)\n",
    "                        if cell != None :\n",
    "                            content = cell.content.replace('\\n', '')\n",
    "                            txtMarkdown = txtMarkdown + f\"{content}\"\n",
    "                            if cell['spans'] != [] :\n",
    "                                cells_offset.append(cell['spans'][0]['offset'])\n",
    "                                \n",
    "                        txtMarkdown = txtMarkdown + \"|\"\n",
    "                        \n",
    "                txtMarkdown = txtMarkdown + f\"\\n\"\n",
    "                \n",
    "                tablesMD.append({'pageNumber':tabe_pageNum, 'spans':table.spans, 'cells':table.cells,'boundingRegions':table.bounding_regions,'markdown':txtMarkdown})\n",
    "            \n",
    "    return tablesMD, cells_offset\n",
    "\n",
    "\n",
    "## Blob Storage에 섹션명으로 된 폴더가 있는 확인하는 함수\n",
    "def folder_exists_in_azure(base_path, section_name):\n",
    "    # 폴더명 리스트 가져오기 위해 blob 이름 확인\n",
    "    folder_names = set()\n",
    "    blobs = prep_container_client.list_blobs(name_starts_with=base_path)\n",
    "    for blob in blobs:\n",
    "        blob_name = blob.name[len(base_path):]  # base_path 이후의 부분만 확인\n",
    "        parts = blob_name.split('/')  # '/' 기준으로 분할\n",
    "        if len(parts) > 1:  # 2개 이상이면 하위 폴더를 의미\n",
    "            folder_names.add(parts[0])\n",
    "\n",
    "    # section_name과 일치하는 폴더가 있는지 확인\n",
    "    return any(name for name in folder_names if name == section_name), folder_names\n",
    "\n",
    "\n",
    "## Table을 데이터프레임으로 변환하는 함수\n",
    "def generate_dataframe_from_table(table):\n",
    "    # 2차원 배열 생성하여 각 셀 저장 공간 마련\n",
    "    rows = [[\"\" for _ in range(table['columnCount'])] for _ in range(table['rowCount'])]\n",
    "\n",
    "    # 각 셀 순회하며 content 채우기\n",
    "    for cell in table['cells']:\n",
    "        row = cell['rowIndex']\n",
    "        col = cell['columnIndex']\n",
    "        content = cell['content']\n",
    "        row_span = cell.get('rowSpan', 1)\n",
    "        col_span = cell.get('columnSpan', 1)\n",
    "\n",
    "        # rowSpan과 columSpan을 순회하며 content 채우기\n",
    "        for r in range(row, row + row_span):\n",
    "            for c in range(col, col + col_span):\n",
    "                rows[r][c] = content\n",
    "\n",
    "    # 데이터프레임으로 변환\n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "## 데이터프레임을 Markdown으로 변환하는 함수\n",
    "def save_df_to_markdown(df):\n",
    "    # 첫 번째 행은 헤더로 설정, 나머지는 테이블로 변환\n",
    "    rows = df.values.tolist()\n",
    "    if not rows:\n",
    "        markdown_str = ''\n",
    "    else:\n",
    "        # 헤더와 데이터 구분\n",
    "        headers = rows[0]\n",
    "        data = rows[1:]\n",
    "       \n",
    "        # Markdown 테이블 생성\n",
    "        markdown_str = '| ' + ' | '.join(str(header) for header in headers) + ' |\\n'\n",
    "        for row in data:\n",
    "            markdown_str += '| ' + ' | '.join(str(cell) for cell in row) + ' |\\n'\n",
    "   \n",
    "    return markdown_str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 텍스트 추출 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM TB_PREP_SOURCE\n",
    "WHERE 1=1\n",
    "    AND IF_CATEGORY_CD = '핸드북'\n",
    "    AND FILE_NM LIKE '%서비스 핸드북%'\n",
    "    AND PREP_TYPE_CD = 'PAGEHEADER'\n",
    "    AND PREP_STATUS_CD = 'N'\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "tmp_df = select_table(mysql_config, query)\n",
    "tmp_df['DI_RUNNING_TIME'] = None  # 실행시간 컬럼 추가\n",
    "\n",
    "\n",
    "raw_container_client = blob_service_client.get_container_client(container=\"raw-data\")\n",
    "pkl_container_client = blob_service_client.get_container_client(container=\"pkl-data\")\n",
    "\n",
    "# pptx를 pdf로 변환 및 pkl 업로드에 필요한 로컬 폴더 생성\n",
    "os.makedirs('DI', exist_ok=True)\n",
    "os.makedirs('pkl', exist_ok=True)\n",
    "\n",
    "for i,row in tmp_df.iterrows():\n",
    "    file_path = row['SOURCE_FILE_PATH']\n",
    "    file_name = row['FILE_NM']\n",
    "    file_name_without_ext = os.path.splitext(file_name)[0]\n",
    "    file_ext = os.path.splitext(file_name)[1]\n",
    "\n",
    "    # pkl 폴더의 모든 파일 삭제\n",
    "    for filename in os.listdir('pkl'):\n",
    "        pkl_path = os.path.join('pkl', filename)\n",
    "        try:\n",
    "            if os.path.isfile(pkl_path):\n",
    "                os.unlink(pkl_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {pkl_path}. Reason: {e}\")\n",
    "    \n",
    "    print('##############################################################')\n",
    "    print(datetime.now())\n",
    "    print(f\"{i} : {file_path}/{file_name}\")\n",
    "    print('##############################################################')\n",
    "    \n",
    "    # 엑셀이면 패스\n",
    "    if file_ext == '.xlsx' or file_ext == '.xls':\n",
    "        continue\n",
    "    \n",
    "    # 확장자가 pptx면 pdf로 변환 후 저장\n",
    "    if file_ext == '.pptx' or file_ext == '.ppt':\n",
    "        pythoncom.CoInitialize()\n",
    "        \n",
    "        for filename in os.listdir('DI'):\n",
    "            pdf_path = os.path.join('DI', filename)\n",
    "            try:\n",
    "                if os.path.isfile(pdf_path):\n",
    "                    os.unlink(pdf_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete {pdf_path}. Reason: {e}\")\n",
    "        \n",
    "        try:\n",
    "            file_client = raw_container_client.get_blob_client(blob=f\"{file_path}/{file_name}\")\n",
    "            \n",
    "            # PPTX 파일을 로컬에 다운로드\n",
    "            with open(f\"DI\\\\{file_name}\", \"wb\") as file:\n",
    "                file.write(file_client.download_blob().readall())\n",
    "                \n",
    "            # PPTX 파일을 PDF로 변환 후 저장\n",
    "            convert(f\"DI\\\\{file_name}\", f\"{os.getcwd()}\\\\DI\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('pass')\n",
    "        \n",
    "        try:\n",
    "            pdf_file_client = raw_container_client.get_blob_client(blob=f\"{file_path}/{file_name_without_ext}.pdf\")\n",
    "            with open(f\"DI\\\\{file_name_without_ext}.pdf\", \"rb\") as f:\n",
    "                pdf_file_client.upload_blob(f, overwrite=True, content_settings=ContentSettings(content_type=\"application/pdf\"))\n",
    "                \n",
    "            file_name = f\"{file_name_without_ext}.pdf\"\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "        \n",
    "        pythoncom.CoUninitialize()\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        sas_url = get_sas_url(blob_config, container_client=raw_container_client, blob_path=f'{file_path}/{file_name}', expiry_time=False)\n",
    "\n",
    "        with urlopen(sas_url) as f:\n",
    "            pdf_data = f.read()\n",
    "            \n",
    "            # DI 사용하는 경우 \n",
    "            poller = document_intelligence_client.begin_analyze_document(\n",
    "                \"prebuilt-layout\",\n",
    "                analyze_request=pdf_data,\n",
    "                features=[DocumentAnalysisFeature.KEY_VALUE_PAIRS],# , DocumentAnalysisFeature.LANGUAGES],\n",
    "                content_type=\"application/octet-stream\",\n",
    "                output_content_format = ContentFormat.MARKDOWN # ContentFormat.TEXT \n",
    "            )\n",
    "\n",
    "        result = poller.result()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        execution_time_seconds = end_time - start_time  # 소스코드 실행 시간 계산 (초 단위)\n",
    "        print(execution_time_seconds)\n",
    "        print(file_name)\n",
    "        \n",
    "        tmp_df.loc[i, 'DI_RUNNING_TIME'] = execution_time_seconds\n",
    "            \n",
    "        with open(f\"pkl\\\\{file_name_without_ext}.pkl\", \"wb\") as f: # 한 번 돌려서 pickle 파일로 저장\n",
    "            pickle.dump(result, f)\n",
    "            \n",
    "        blob_client = pkl_container_client.get_blob_client(blob=f\"{file_path}/{file_name_without_ext}.pkl\")\n",
    "        \n",
    "        with open(f\"pkl\\\\{file_name_without_ext}.pkl\", \"rb\") as data:\n",
    "            blob_client.upload_blob(data, overwrite=True, content_settings=ContentSettings(content_type=\"application/octet-stream\"))\n",
    "            \n",
    "        update_query = f\"\"\"\n",
    "            UPDATE TB_PREP_SOURCE\n",
    "            SET PREP_STATUS_CD = 'C'\n",
    "                , UPDATE_PGM = 'PAGEHEADER 전처리'\n",
    "                , UPDATE_DTTM = SYSDATE()\n",
    "            WHERE 1=1\n",
    "                AND IF_CATEGORY_CD = '{row['IF_CATEGORY_CD']}'\n",
    "                AND CONTENTS_ID    = '{row['CONTENTS_ID']}'\n",
    "                AND ITEM_ID        = '{row['ITEM_ID']}'\n",
    "            ;\n",
    "        \"\"\"\n",
    "        commit_query(mysql_config, update_query)\n",
    "        print('Compelete!!!!!!!!!!!!!!!!')\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"{str(e)}\")\n",
    "        \n",
    "        error_msg = f\"{str(e)}\"\n",
    "        \n",
    "        update_query = f\"\"\"\n",
    "            UPDATE TB_PREP_SOURCE\n",
    "            SET PREP_STATUS_CD = 'E'\n",
    "                , PREP_ERROR_MSG = '{error_msg}'\n",
    "                , UPDATE_PGM = 'PAGEHEADER 전처리'\n",
    "                , UPDATE_DTTM = SYSDATE()\n",
    "            WHERE 1=1\n",
    "                AND IF_CATEGORY_CD = '{row['IF_CATEGORY_CD']}'\n",
    "                AND CONTENTS_ID    = '{row['CONTENTS_ID']}'\n",
    "                AND ITEM_ID        = '{row['ITEM_ID']}'\n",
    "            ;\n",
    "        \"\"\"\n",
    "        commit_query(mysql_config, update_query)\n",
    "        continue\n",
    "    \n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 섹션 분할 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_container_client = blob_service_client.get_container_client(container=\"raw-data\")\n",
    "prep_container_client = blob_service_client.get_container_client(container=\"prep-data\")\n",
    "web_client = blob_service_client_prod2.get_container_client(container=\"$web\")\n",
    "\n",
    "\n",
    "## 페이지별 pageheader 추출할 때 제외할 날짜 텍스트 입력\n",
    "except_date = '20240829'\n",
    "\n",
    "\n",
    "## 전처리 대상 load \n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    *\n",
    "FROM TB_PREP_SOURCE\n",
    "WHERE 1=1\n",
    "    AND IF_CATEGORY_CD = '핸드북'\n",
    "    AND FILE_NM LIKE '%서비스 핸드북%'\n",
    "    AND PREP_TYPE_CD = 'PAGEHEADER'\n",
    "    AND PREP_STATUS_CD = 'C'\n",
    ";\n",
    "\"\"\"\n",
    "tb_if_prep_files_source = select_table(mysql_config, query)\n",
    "\n",
    "## pkl 블롭 파일 목록\n",
    "pkl_container_client = blob_service_client.get_container_client(container=\"pkl-data\")\n",
    "pkl_blob_list = [i for i in pkl_container_client.list_blob_names() if \".pkl\" in i]\n",
    "## 테이블 활용해서 pkl 파일 위치 찾기\n",
    "pkl_path_df = tb_if_prep_files_source[['IF_CATEGORY_CD', 'CONTENTS_ID', 'ITEM_ID'\n",
    "                                       ,'FILE_NM', 'SOURCE_FILE_PATH', 'RESULT_FILE_PATH'\n",
    "                                       , 'PRODUCT_LVL1_CD', 'PRODUCT_LVL2_CD', 'PRODUCT_LVL3_CD'\n",
    "                                       , 'PROD_MODEL_CD', 'SALES_MODEL_CD']].drop_duplicates()\n",
    "\n",
    "## 제외 Text load\n",
    "query = f\"\"\"\n",
    "SELECT TRIM(EXCEPT_TEXT_VAL) AS EXCEPT_TEXT_VAL\n",
    "FROM TB_EXCEPT_TEXT_MST\n",
    "WHERE USE_FLAG = 'Y'\n",
    "ORDER BY LENGTH(EXCEPT_TEXT_VAL) DESC\n",
    ";\n",
    "\"\"\"\n",
    "TB_MST_PREP_EXCEPT_TEXT = select_table(mysql_config, query)\n",
    "except_texts = TB_MST_PREP_EXCEPT_TEXT['EXCEPT_TEXT_VAL'].to_list()\n",
    "\n",
    "pkl_path_df['CT_SECTION_SPLIT_RUNNING_TIME'] = None\n",
    "\n",
    "\n",
    "# 파일 하나씩 loop\n",
    "for idx in tqdm(pkl_path_df.index):\n",
    "    error_msg = ''\n",
    "    \n",
    "    idx_row  = pkl_path_df.loc[idx]\n",
    "    category_cd = idx_row['IF_CATEGORY_CD']\n",
    "    contents_id = idx_row['CONTENTS_ID']\n",
    "    item_id = idx_row['ITEM_ID']\n",
    "    prod_model_cd = idx_row['PROD_MODEL_CD']\n",
    "    sales_model_cd = idx_row['SALES_MODEL_CD']\n",
    "    \n",
    "    file_nm  = idx_row['FILE_NM']\n",
    "    file_nm_no_ext = os.path.splitext(file_nm)[0]\n",
    "    file_type = file_nm.split('.')[-1]\n",
    "\n",
    "    src_fpath = idx_row['SOURCE_FILE_PATH']\n",
    "    result_path = idx_row['RESULT_FILE_PATH']\n",
    "\n",
    "    lvl1_cd = idx_row['PRODUCT_LVL1_CD']\n",
    "    lvl2_cd = idx_row['PRODUCT_LVL2_CD']\n",
    "    lvl3_cd = idx_row['PRODUCT_LVL3_CD']\n",
    "\n",
    "    print('##############################################################')\n",
    "    print(f'################### {src_fpath}/{file_nm} ###################')\n",
    "    print('##############################################################')\n",
    "    \n",
    "    cur_pkl_blob_list = [pkl_blob for pkl_blob in pkl_blob_list if f\"{src_fpath}/{file_nm_no_ext}.pkl\" == pkl_blob]\n",
    "    if (len(cur_pkl_blob_list) == 0) | (len(cur_pkl_blob_list) > 1) :\n",
    "        print(f\"pickle 파일이 없거나 너무 많습니다.\")\n",
    "        print(f\"PKL FILE LIST : {cur_pkl_blob_list}\")\n",
    "        continue\n",
    "    \n",
    "    ## Load Pkl\n",
    "    pkl_blob_client = pkl_container_client.get_blob_client(f\"{src_fpath}/{file_nm_no_ext}.pkl\")\n",
    "    pkl_db_bytes   = pkl_blob_client.download_blob().readall()\n",
    "    pkl_blob_stream = io.BytesIO(pkl_db_bytes)\n",
    "    di_result = pickle.load(pkl_blob_stream)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        sas_url = get_sas_url(blob_config, container_client=pdf_container_client, blob_path=f\"{src_fpath}/{file_nm_no_ext}.pdf\", expiry_time=False)\n",
    "        request_blob = requests.get(sas_url)\n",
    "        filestream = io.BytesIO(request_blob.content)\n",
    "        doc = pdfplumber.open(filestream)\n",
    "        doc2 = fitz.open(stream=filestream, filetype=\"pdf\")\n",
    "    except:\n",
    "        sas_url = get_sas_url(blob_config, container_client=pdf_container_client, blob_path=f\"{src_fpath}/{file_nm_no_ext}.PDF\", expiry_time=False)\n",
    "        request_blob = requests.get(sas_url)\n",
    "        filestream = io.BytesIO(request_blob.content)\n",
    "        doc = pdfplumber.open(filestream)\n",
    "        doc2 = fitz.open(stream=filestream, filetype=\"pdf\")\n",
    "\n",
    "\n",
    "    # size_min, size_max를 가져오기 위해 RESULT 테이블 SELECT\n",
    "    query = f\"\"\"\n",
    "        SELECT \n",
    "            SEQ_ID, SIZE_MIN, SIZE_MAX\n",
    "        FROM TB_PREP_RESULT\n",
    "        WHERE 1=1\n",
    "            AND IF_CATEGORY_CD = '{idx_row['IF_CATEGORY_CD']}'\n",
    "            AND FILE_NM LIKE '%서비스 핸드북%'\n",
    "            AND IF_TYPE_CD != 'D'\n",
    "        ;\n",
    "    \"\"\"\n",
    "    handbook_size_df = select_table(mysql_config, query)\n",
    "\n",
    "\n",
    "    # 결과를 insert할 데이터프레임 구조 정의\n",
    "    query = f\"\"\"\n",
    "    SELECT COLUMN_NAME\n",
    "    FROM INFORMATION_SCHEMA.COLUMNS\n",
    "    WHERE TABLE_NAME = 'TB_PREP_RESULT_STG'\n",
    "    AND TABLE_SCHEMA = 'QBOTDB'\n",
    "    ORDER BY ORDINAL_POSITION ASC;\n",
    "    \"\"\"\n",
    "    result_df = select_table(mysql_config, query)\n",
    "    result_list = result_df['COLUMN_NAME'].to_list()\n",
    "    df_data = {col: [None]*len(doc.pages) for col in result_list}\n",
    "    result_df = pd.DataFrame(df_data)\n",
    "    \n",
    "    # result_df에 고정값 세팅\n",
    "    result_df['IF_CATEGORY_CD'] = category_cd\n",
    "    result_df['CONTENTS_ID'] = contents_id\n",
    "    result_df['ITEM_ID'] = item_id\n",
    "    result_df['FILE_NM'] = file_nm_no_ext\n",
    "    result_df['MODEL_USE_FLAG'] = 'Y'\n",
    "    result_df['PRODUCT_LVL1_CD'] = lvl1_cd\n",
    "    result_df['PRODUCT_LVL2_CD'] = lvl2_cd\n",
    "    result_df['PRODUCT_LVL3_CD'] = lvl3_cd\n",
    "    result_df['PROD_MODEL_CD'] = prod_model_cd\n",
    "    result_df['SALES_MODEL_CD'] = sales_model_cd\n",
    "    result_df['IF_FLAG'] = 'N'\n",
    "    result_df['CREATE_DTTM'] = datetime.now()\n",
    "    result_df['CREATE_PGM'] = 'PAGEHEADER 전처리'\n",
    "    result_df['IF_TYPE_CD'] = 'U'\n",
    "    \n",
    "    # 기존에 해당 파일을 전처리 했으면 blob에서 삭제\n",
    "    result_file_blobs = prep_container_client.list_blobs(name_starts_with=f'{result_path}/{file_nm_no_ext}/')\n",
    "    for result_file_blob in result_file_blobs:\n",
    "        result_blob_client = prep_container_client.get_blob_client(result_file_blob)\n",
    "        result_blob_client.delete_blob()\n",
    "\n",
    "    result_df_idx = 0\n",
    "     \n",
    "    prc_str_time = time.time()\n",
    "    \n",
    "    # 페이지별로 loop\n",
    "    for page in di_result.pages:\n",
    "        section_file_name = None\n",
    "        saveText = \"\"\n",
    "        saveText_bytes = None\n",
    "        saveText_base64 = None\n",
    "        size_min = None\n",
    "        size_max = None\n",
    "        pageContents = []\n",
    "\n",
    "        table_markdown_flag = False\n",
    "\n",
    "        tables = []\n",
    "        cells_offset = []  \n",
    "        \n",
    "        pageHeader = None      # PageHeader\n",
    "        title = None           # title\n",
    "        sectionHeading = None  # SectionHeading\n",
    "        first_text = None      # 첫 번째로 추출되는 문자\n",
    "        \n",
    "        pageNum = page.page_number\n",
    "        pageParagraphs = get_page_paragraphs(di_result.paragraphs, pageNum, except_texts)\n",
    "        \n",
    "        print('==============================================================')\n",
    "        print(f'================== 현재 pageNum : {pageNum} ==================')\n",
    "        print('==============================================================')\n",
    "        \n",
    "        if pageParagraphs == [] or pageParagraphs == None:\n",
    "            continue\n",
    "            \n",
    "        # PageHeader 추출\n",
    "        for para in pageParagraphs:\n",
    "            page_content = para.content\n",
    "            \n",
    "            ## 제외 Text 제외\n",
    "            for except_text in except_texts:\n",
    "                if except_text in page_content:\n",
    "                    page_content = page_content.replace(except_text, '')\n",
    "            \n",
    "            page_content = re.sub(r'[^가-힣a-zA-Z0-9\\s]', '', page_content)\n",
    "            page_content = page_content.replace('\\n', '').replace('\\\\', '') \\\n",
    "                    .replace('/', '').strip()\n",
    "\n",
    "            page_content_chk = page_content.replace(' ', '')\n",
    "\n",
    "            if '부표' in page_content_chk or except_date in page_content_chk:\n",
    "                continue\n",
    "\n",
    "            if '수리유형' in page_content_chk or '수리코드' in page_content_chk:\n",
    "                table_markdown_flag = True\n",
    "\n",
    "            ## Pageheader 추출\n",
    "            if page_content_chk != '' :\n",
    "                if para.role == 'pageHeader':\n",
    "                    if pageHeader == None :\n",
    "                        pageHeader = page_content\n",
    "                if para.role == 'title':\n",
    "                    if title == None :\n",
    "                        title = page_content\n",
    "                if para.role == 'sectionHeading':\n",
    "                    if sectionHeading == None :\n",
    "                        sectionHeading = page_content\n",
    "                if first_text == None :\n",
    "                    first_text = page_content\n",
    "                    first_text = first_text[:990]\n",
    "        \n",
    "        if pageHeader != None :    # PageHeader가 잡힌 경우\n",
    "            section_file_name = pageHeader\n",
    "        elif title != None :       # title이 잡힌 경우\n",
    "            section_file_name = title\n",
    "        elif sectionHeading != None :  # sectionHeading이 잡힌 경우\n",
    "            section_file_name = sectionHeading\n",
    "        elif first_text != None :  # 첫 번째로 문자가 추출된 경우\n",
    "            section_file_name = first_text\n",
    "        section_file_name = section_file_name[:990]\n",
    "        \n",
    "        # 섹션 분할\n",
    "        pageContents.append(f\"{pageNum} : {section_file_name}\")\n",
    "        \n",
    "        if di_result.tables != None:\n",
    "            tables, cells_offset = get_page_table(di_result.tables, pageNum)\n",
    "\n",
    "            table_idx = 0\n",
    "            for table in di_result.tables:\n",
    "                convert_df = None\n",
    "                new_markdown = None\n",
    "                \n",
    "                tabe_pageNum = table.get(\"boundingRegions\", [])[0].get(\"pageNumber\")\n",
    "\n",
    "                if tabe_pageNum != pageNum:\n",
    "                    continue\n",
    "                \n",
    "                convert_df = generate_dataframe_from_table(table)\n",
    "\n",
    "                if section_file_name.startswith('제품별 수리코드') and table_markdown_flag:\n",
    "                    # 공백인 셀의 행, 열 좌표 추출\n",
    "                    empty_cells = [(i, j) for i in range(len(convert_df)) for j in [3, 4, 5] if convert_df.iloc[i, j] == '']\n",
    "\n",
    "                    # 동일한 수리유형과 수리코드를 가진 다른 행에서 값 추출 및 매핑\n",
    "                    for i, j in empty_cells:\n",
    "                        repair_type = convert_df.iloc[i, 0]\n",
    "                        repair_code = convert_df.iloc[i, 1]\n",
    "\n",
    "                        # 공백이 없는 행들 중 동일한 수리유형과 수리코드를 가진 행 찾기\n",
    "                        matching_rows = convert_df[\n",
    "                            (convert_df[0] == repair_type) & \n",
    "                            (convert_df[1] == repair_code) & \n",
    "                            (convert_df.index != i) & \n",
    "                            (convert_df.iloc[:, j] != '')  # 해당 열에 공백이 없는 행만 선택\n",
    "                        ]\n",
    "\n",
    "                        if not matching_rows.empty:\n",
    "                            # j열의 값을 매핑\n",
    "                            convert_df.iloc[i, j] = matching_rows.iloc[0, j]\n",
    "        \n",
    "                new_markdown = save_df_to_markdown(convert_df)\n",
    "                tables[table_idx]['markdown'] = new_markdown\n",
    "\n",
    "                table_idx += 1\n",
    "\n",
    "        \n",
    "        for para in pageParagraphs :\n",
    "            if para['spans'] != []:\n",
    "                para_offset = para['spans'][0]['offset']\n",
    "                \n",
    "                if len(tables) > 0 :\n",
    "                    if tables[0]['spans'] != []:\n",
    "                        table_offset = tables[0]['spans'][0]['offset']\n",
    "                        if para_offset >= table_offset :\n",
    "                            pageContents.append(tables[0]['markdown'])\n",
    "                            del tables[0]\n",
    "                    \n",
    "                if para_offset not in cells_offset:\n",
    "                    pageContents.append(para['content'])\n",
    "                    \n",
    "        for i in range(len(pageContents)) :\n",
    "            content = pageContents[i]\n",
    "            \n",
    "            for except_text in except_texts:\n",
    "                if except_text in content:\n",
    "                    content = content.replace(except_text, '')\n",
    "                    \n",
    "            saveText = saveText + content + '\\n'\n",
    "            pageContents[i] = content\n",
    "        \n",
    "        ## Blob 저장 (저장할 Text)\n",
    "        exists, folder_names = folder_exists_in_azure(f\"{result_path}/{file_nm_no_ext}/\", section_file_name)\n",
    "        \n",
    "        if exists:\n",
    "            folder_name = f'{result_path}/{file_nm_no_ext}/' + [name for name in folder_names if name == section_file_name][0]\n",
    "        else:\n",
    "            folder_name = f\"{result_path}/{file_nm_no_ext}/{section_file_name}\"\n",
    "            \n",
    "        save_blob_name = f\"{folder_name}/{str(pageNum).zfill(3)}_{section_file_name}.txt\"\n",
    "        print(save_blob_name)\n",
    "        \n",
    "        ## result_df에 필요한 정보 추가\n",
    "        result_df.loc[result_df_idx, 'SECTION_NM'] = folder_name.split('/')[-1]\n",
    "        result_df.loc[result_df_idx, 'SECTION_FILE_NM'] = save_blob_name.split('/')[-1]\n",
    "        result_df.loc[result_df_idx, 'FULL_FILE_PATH'] = save_blob_name\n",
    "        result_df.loc[result_df_idx, 'SEQ_ID'] = pageNum\n",
    "        result_df.loc[result_df_idx, 'PAGE_NUM'] = pageNum\n",
    "        result_df.loc[result_df_idx, 'PAGE_NUMS'] = pageNum\n",
    "\n",
    "        # 해당 seq_id에 해당하는 SIZE_MIN, SIZE_MAX 가져오기\n",
    "        size_result = handbook_size_df.loc[handbook_size_df['SEQ_ID'] == pageNum, ['SIZE_MIN', 'SIZE_MAX']]\n",
    "\n",
    "        if not size_result.empty:\n",
    "            size_min = size_result.iloc[0]['SIZE_MIN']\n",
    "            size_max = size_result.iloc[0]['SIZE_MAX']\n",
    "\n",
    "            # SIZE_MIN, SIZE_MAX 설정\n",
    "            result_df.loc[result_df_idx, 'SIZE_MIN'] = size_min\n",
    "            result_df.loc[result_df_idx, 'SIZE_MAX'] = size_max\n",
    "            \n",
    "        # 각 페이지 txt 파일 적재  --> Blob 에 적재\n",
    "        blob_client = prep_container_client.get_blob_client(blob=save_blob_name)\n",
    "        blob_client.upload_blob(saveText, overwrite=True)\n",
    "        \n",
    "        saveText_bytes = saveText.encode('utf-8')\n",
    "        saveText_base64 = base64.b64encode(saveText_bytes).decode('utf-8')\n",
    "        result_df.loc[result_df_idx, 'RESULT_TEXT'] = saveText_base64\n",
    "        \n",
    "        # 각 페이지에서 image가 차지하는 비율 계산\n",
    "        target = doc.pages[pageNum-1]\n",
    "        \n",
    "        text_length_sum = 0\n",
    "        image_sum = 0\n",
    "        image_ratio = 0\n",
    "        \n",
    "        # 페이지 넓이 계산\n",
    "        page_area = round(target.width*target.height, 2)\n",
    "        \n",
    "        # 텍스트 개수 계산\n",
    "        for texts in target.extract_words():\n",
    "            text = texts['text']\n",
    "            length = len(text)\n",
    "            text_length_sum += length\n",
    "            \n",
    "        result_df.loc[result_df_idx, 'TEXT_CNT'] = text_length_sum\n",
    "        \n",
    "        if target.images != []:\n",
    "            # Image 넓이 계산\n",
    "            for images in target.images:\n",
    "                width = round(images['width'], 2)\n",
    "                height = round(images['height'], 2)\n",
    "                area = round(width*height, 2)\n",
    "                image_sum += area\n",
    "            image_sum = round(image_sum, 2)\n",
    "            image_ratio = round(image_sum / page_area, 2)\n",
    "            \n",
    "            result_df.loc[result_df_idx, 'IMAGE_RATIO'] = image_ratio\n",
    "\n",
    "            caption_start = []\n",
    "            pad = 15\n",
    "            dpi = 200\n",
    "            seq = 1\n",
    "            \n",
    "            # 캡션 텍스트 좌표 확인\n",
    "            for text in target.extract_words():\n",
    "                if '[이미지_' in text['text']:\n",
    "                    caption_start.append([text['x0'], text['bottom'], text['text']])\n",
    "            \n",
    "            try:\n",
    "                # 페이지별 이미지 추출\n",
    "                for image in target.images:\n",
    "                    for i in range(len(caption_start)):\n",
    "                        start_x = caption_start[i][0]\n",
    "                        start_y = caption_start[i][1]\n",
    "                        tag_name = caption_start[i][2]\n",
    "                        \n",
    "                        if (start_x-pad <= image['x0'] <= start_x+pad) and (start_y <= image['top'] <= start_y+pad):\n",
    "                            page = doc2[pageNum-1]\n",
    "                            page.set_cropbox([image['x0'], image['top'], image['x1'], image['bottom']])\n",
    "                            pixmap = page.get_pixmap(matrix=fitz.Matrix(dpi/72, dpi/72))\n",
    "                            # PNG 형식으로 이미지 데이터 추출\n",
    "                            img_data = pixmap.tobytes(\"png\")\n",
    "                            # Blob Storage에 업로드\n",
    "                            img_blob_client = prep_container_client.get_blob_client(f\"{folder_name}/images/{str(pageNum).zfill(3)}_{str(seq).zfill(3)}_{tag_name}.png\")\n",
    "                            img_blob_client.upload_blob(img_data, overwrite=True, content_type=\"image/png\")\n",
    "                            \n",
    "                            seq += 1\n",
    "            except Exception as e:\n",
    "                print(f\"{str(e)}\")\n",
    "                \n",
    "                result_df_idx += 1\n",
    "                continue\n",
    "                    \n",
    "        result_df_idx += 1\n",
    "\n",
    "    doc2.close()\n",
    "    \n",
    "    ## PAGE_NUM이 NULL인 행 삭제\n",
    "    result_df = result_df.dropna(subset=['PAGE_NUM'])\n",
    "\n",
    "    # NaN을 None으로 변환\n",
    "    result_df = result_df.replace({np.nan: None})\n",
    "     \n",
    "    ## HTML 생성\n",
    "    try:\n",
    "        sas_url = get_sas_url(blob_config, container_client=pdf_container_client, blob_path=f\"{src_fpath}/{file_nm_no_ext}.pdf\", expiry_time=False)\n",
    "        request_blob = requests.get(sas_url)\n",
    "        filestream = io.BytesIO(request_blob.content)\n",
    "        doc = fitz.open(stream=filestream, filetype=\"pdf\")\n",
    "    except:\n",
    "        sas_url = get_sas_url(blob_config, container_client=pdf_container_client, blob_path=f\"{src_fpath}/{file_nm_no_ext}.PDF\", expiry_time=False)\n",
    "        request_blob = requests.get(sas_url)\n",
    "        filestream = io.BytesIO(request_blob.content)\n",
    "        doc = fitz.open(stream=filestream, filetype=\"pdf\")\n",
    "    print(f\"[#] 현재 문서 : {file_nm_no_ext}.pdf\")\n",
    "    print(f\"page 수 : {len(doc)}\")\n",
    "\n",
    "    # pdf 저장 경로 사용해서 적재하는경우\n",
    "    save_folder = f'{result_path}/{file_nm_no_ext}'\n",
    "\n",
    "    for i, page in tqdm(enumerate(doc)):\n",
    "        svg = page.get_svg_image(matrix=fitz.Identity)\n",
    "        svg_bytes  = svg.encode('utf-8')\n",
    "        img_data = io.BytesIO(svg_bytes)\n",
    "        img_data.seek(0)\n",
    "        img_blob_client = web_client.get_blob_client(f\"{save_folder}/images/{i+1}.svg\")\n",
    "        img_blob_client.upload_blob(img_data, content_type=\"image/svg+xml\", overwrite=True)\n",
    "    doc.close()\n",
    "\n",
    "    img_path       = f\"{save_folder}/images/\"\n",
    "    img_bname_list = web_client.list_blobs(name_starts_with = img_path) # 적재된 image 목록 \n",
    "    img_bname_list = [i.name for i in img_bname_list if i.name.endswith('.svg')]\n",
    "\n",
    "    if len(img_bname_list) == 0:\n",
    "        print(\"적재된 Image 파일 없음\")\n",
    "    else : \n",
    "        blob_client = web_client.get_blob_client(f'{result_path}/{file_nm_no_ext}')\n",
    "        root_folder = f\"{blob_client.url}/images\"\n",
    "\n",
    "        html_content = \"<html><head></head><body style='background-color: white;'></body>\\n\"\n",
    "        html_content += \"<script>\\n\"\n",
    "        \n",
    "        if file_type == 'ppt' or file_type == 'pptx':\n",
    "            html_content += f\"\"\"\n",
    "const rootFolder = '{root_folder}'\n",
    "const sas_key = 'sv=2023-11-03&st=2024-07-24T08%3A08%3A12Z&se=9999-12-31T12%3A00%3A00Z&sr=c&sp=rl&sig=vfHfN9ax60XVqxsZ5ae2VUjAomSwqZGKOLrptQiDKko%3D'\n",
    "const hash = window.location.hash;\n",
    "const page = hash.match(/#page(\\d+)/)[1];\n",
    "\n",
    "if(page) {{\n",
    "    let pageInt = parseInt(page)\n",
    "    let startPage = pageInt - 5;\n",
    "    let endPage = pageInt + 5;\n",
    "\n",
    "    let body = document.body;\n",
    "    for ( i = startPage; i <= endPage; i++) {{\n",
    "\n",
    "        if(i >= 1){{\n",
    "            \n",
    "            div = document.createElement('div');\n",
    "            div.setAttribute('id', 'page'+i);\n",
    "            div.setAttribute('style', 'text-align: center;');\n",
    "\n",
    "            let img = document.createElement('img');\n",
    "            let src = rootFolder+'/'+i+'.svg'+'?'+sas_key\n",
    "            img.setAttribute('src', src)\n",
    "            img.setAttribute('style', 'display: inline-block; margin:10px auto;width:1500px;');\n",
    "            img.onerror = function() {{\n",
    "                img.remove();\n",
    "            }};\n",
    "            \n",
    "            if(img) {{\n",
    "                div.appendChild(img);\n",
    "                body.appendChild(div);\n",
    "            }}\n",
    "        }}\n",
    "    }}\n",
    "}}\n",
    "            \"\"\"\n",
    "\n",
    "        else:\n",
    "            \n",
    "            html_content += f\"\"\"\n",
    "const rootFolder = '{root_folder}'\n",
    "const sas_key = 'sv=2023-11-03&st=2024-07-24T08%3A08%3A12Z&se=9999-12-31T12%3A00%3A00Z&sr=c&sp=rl&sig=vfHfN9ax60XVqxsZ5ae2VUjAomSwqZGKOLrptQiDKko%3D'\n",
    "const hash = window.location.hash;\n",
    "const page = hash.match(/#page(\\d+)/)[1];\n",
    "\n",
    "if(page) {{\n",
    "    let pageInt = parseInt(page)\n",
    "    let startPage = pageInt - 5;\n",
    "    let endPage = pageInt + 5;\n",
    "\n",
    "    let body = document.body;\n",
    "    for ( i = startPage; i <= endPage; i++) {{\n",
    "\n",
    "        if(i >= 1){{\n",
    "            \n",
    "            div = document.createElement('div');\n",
    "            div.setAttribute('id', 'page'+i);\n",
    "            div.setAttribute('style', 'text-align: center;');\n",
    "\n",
    "            let img = document.createElement('img');\n",
    "            let src = rootFolder+'/'+i+'.svg'+'?'+sas_key\n",
    "            img.setAttribute('src', src)\n",
    "            img.setAttribute('style', 'display: inline-block; margin:10px auto;width:70%;');\n",
    "            img.onerror = function() {{\n",
    "                img.remove();\n",
    "            }};\n",
    "            \n",
    "            if(img) {{\n",
    "                div.appendChild(img);\n",
    "                body.appendChild(div);\n",
    "            }}\n",
    "        }}\n",
    "    }}\n",
    "}}\n",
    "            \"\"\"\n",
    "\n",
    "        html_content += \"\\n</script></html>\"\n",
    "            \n",
    "        html_bname = f\"{save_folder}/{file_nm_no_ext}.html\"\n",
    "        html_blob_client = web_client.get_blob_client(html_bname)\n",
    "        html_blob_client.upload_blob(html_content, content_type=(\"text/html\"), overwrite=True)\n",
    "        \n",
    "        ## SAS Link 생성\n",
    "        html_sas_url = get_sas_url(blob_config_prod2, web_client, html_bname, expiry_time=False)\n",
    "\n",
    "        for result_idx, result_row in result_df.iterrows():\n",
    "            pnum = result_row['PAGE_NUM']\n",
    "            result_df.loc[result_idx, 'HTML_SAS_URL'] = html_sas_url + '#page' + str(pnum)\n",
    "        \n",
    "    prc_running_time = time.time() - prc_str_time\n",
    "    \n",
    "    pkl_path_df.loc[idx, 'CT_SECTION_SPLIT_RUNNING_TIME'] = prc_running_time\n",
    "\n",
    "    try:\n",
    "        # INSERT RESULT_STG_BACKUP\n",
    "        backup_query = f\"\"\"\n",
    "            INSERT INTO TB_PREP_RESULT_STG_BACKUP\n",
    "            (BACKUP_DTTM, IF_CATEGORY_CD, CONTENTS_ID, ITEM_ID, SEQ_ID, FILE_NM, SECTION_NM,\n",
    "            SECTION_FILE_NM, PAGE_NUM, PAGE_NUMS, FULL_FILE_PATH, HTML_SAS_URL, MODEL_USE_FLAG, PRODUCT_LVL1_CD,\n",
    "            PRODUCT_LVL2_CD, PRODUCT_LVL3_CD, PROD_MODEL_CD, SALES_MODEL_CD, SYMP_CODE_ONE, SYMP_CODE_TWO, SYMP_CODE_THREE, IF_FLAG, IF_DTTM,\n",
    "            CREATE_DTTM, CREATE_PGM, UPDATE_DTTM, UPDATE_PGM, RESULT_TEXT, IMAGE_RATIO,\n",
    "            TEXT_CNT, SIZE_MIN, SIZE_MAX, DOCS_DATE, IF_TYPE_CD)\n",
    "            SELECT SYSDATE() AS BACKUP_DTTM,\n",
    "            IF_CATEGORY_CD, CONTENTS_ID, ITEM_ID, SEQ_ID, FILE_NM, SECTION_NM,\n",
    "            SECTION_FILE_NM, PAGE_NUM, PAGE_NUMS, FULL_FILE_PATH, HTML_SAS_URL, MODEL_USE_FLAG, PRODUCT_LVL1_CD,\n",
    "            PRODUCT_LVL2_CD, PRODUCT_LVL3_CD, PROD_MODEL_CD, SALES_MODEL_CD, SYMP_CODE_ONE, SYMP_CODE_TWO, SYMP_CODE_THREE, IF_FLAG, IF_DTTM,\n",
    "            CREATE_DTTM, CREATE_PGM, UPDATE_DTTM, UPDATE_PGM, RESULT_TEXT, IMAGE_RATIO,\n",
    "            TEXT_CNT, SIZE_MIN, SIZE_MAX, DOCS_DATE, IF_TYPE_CD\n",
    "            FROM TB_PREP_RESULT_STG\n",
    "            WHERE 1=1\n",
    "                AND IF_CATEGORY_CD = '{idx_row['IF_CATEGORY_CD']}'\n",
    "                AND CONTENTS_ID    = '{idx_row['CONTENTS_ID']}'\n",
    "                AND ITEM_ID        = '{idx_row['ITEM_ID']}'\n",
    "            ;\n",
    "        \"\"\"\n",
    "        commit_query(mysql_config, backup_query)\n",
    "        \n",
    "        # DELETE RESULT_STG\n",
    "        delete_query = f\"\"\"\n",
    "            DELETE\n",
    "            FROM TB_PREP_RESULT_STG\n",
    "            WHERE 1=1\n",
    "                AND IF_CATEGORY_CD = '{idx_row['IF_CATEGORY_CD']}'\n",
    "                AND CONTENTS_ID    = '{idx_row['CONTENTS_ID']}'\n",
    "                AND ITEM_ID        = '{idx_row['ITEM_ID']}'\n",
    "            ;\n",
    "        \"\"\"\n",
    "        commit_query(mysql_config, delete_query)\n",
    "        \n",
    "        \n",
    "        # INSERT RESULT_STG\n",
    "        query = f\"\"\"\n",
    "            INSERT INTO TB_PREP_RESULT_STG\n",
    "            (IF_CATEGORY_CD, CONTENTS_ID, ITEM_ID, SEQ_ID, FILE_NM, SECTION_NM,\n",
    "            SECTION_FILE_NM, PAGE_NUM, PAGE_NUMS, FULL_FILE_PATH, HTML_SAS_URL, MODEL_USE_FLAG, PRODUCT_LVL1_CD,\n",
    "            PRODUCT_LVL2_CD, PRODUCT_LVL3_CD, PROD_MODEL_CD, SALES_MODEL_CD, SYMP_CODE_ONE, SYMP_CODE_TWO, SYMP_CODE_THREE, IF_FLAG, IF_DTTM,\n",
    "            CREATE_DTTM, CREATE_PGM, UPDATE_DTTM, UPDATE_PGM, RESULT_TEXT, IMAGE_RATIO,\n",
    "            TEXT_CNT, SIZE_MIN, SIZE_MAX, DOCS_DATE, IF_TYPE_CD)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s,\n",
    "            %s, %s, %s, %s, %s, %s, %s,\n",
    "            %s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
    "            %s, %s, %s, %s, FROM_BASE64(%s), %s,\n",
    "            %s, %s, %s, %s, %s)\n",
    "            ;\n",
    "        \"\"\"\n",
    "        vals = list(result_df.itertuples(index=False, name=None))\n",
    "        commit_query_w_vals(mysql_config, query, vals, True)\n",
    "        \n",
    "        # UPDATE SOURCE\n",
    "        update_query = f\"\"\"\n",
    "            UPDATE TB_PREP_SOURCE\n",
    "            SET PREP_STATUS_CD = 'Y'\n",
    "                , UPDATE_PGM = 'PAGEHEADER 전처리'\n",
    "                , UPDATE_DTTM = SYSDATE()\n",
    "            WHERE 1=1\n",
    "                AND IF_CATEGORY_CD = '{idx_row['IF_CATEGORY_CD']}'\n",
    "                AND CONTENTS_ID    = '{idx_row['CONTENTS_ID']}'\n",
    "                AND ITEM_ID        = '{idx_row['ITEM_ID']}'\n",
    "            ;\n",
    "        \"\"\"\n",
    "        commit_query(mysql_config, update_query)\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_msg += f\"{str(e)}\\n\"\n",
    "        \n",
    "        print('DB INSERT 실패')\n",
    "        update_query = f\"\"\"\n",
    "            UPDATE TB_PREP_SOURCE\n",
    "            SET PREP_STATUS_CD = 'E'\n",
    "                , PREP_ERROR_MSG = '{error_msg}'\n",
    "                , UPDATE_PGM = 'PAGEHEADER 전처리'\n",
    "                , UPDATE_DTTM = SYSDATE()\n",
    "            WHERE 1=1\n",
    "                AND IF_CATEGORY_CD = '{idx_row['IF_CATEGORY_CD']}'\n",
    "                AND CONTENTS_ID    = '{idx_row['CONTENTS_ID']}'\n",
    "                AND ITEM_ID        = '{idx_row['ITEM_ID']}'\n",
    "            ;\n",
    "        \"\"\"\n",
    "        commit_query(mysql_config, update_query)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
